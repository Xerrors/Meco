---
title: 命名实体识别资料整理与知识点记录
date: 2021-07-21 13:02:51
permalink: /ner-beginner-note/
cover: https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210803125619.png
tags: 
- 人工智能
- NLP
categories: 人工智能
---
## 1. 命名实体识别 NER

一篇很好的综述：[[1812.09449] A Survey on Deep Learning for Named Entity Recognition (arxiv.org)](https://arxiv.org/abs/1812.09449)

这是我写的思维导图笔记：[《命名实体识别》](https://shimo.im/mindmaps/3PCHWJCYkw98cgvR/)

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210728213903-命名实体识别.jpeg)

### NLP四大任务

> 为什么说流水的NLP铁打的NER？NLP四大任务嘛，分类、生成、序列标注、句子对标注。分类任务，面太广了，万物皆可分类，各种方法层出不穷；句子对标注，经常是体现人工智能（zhang）对人类语言理解能力的标准秤，孪生网络、DSSM、ESIM 各种模型一年年也是秀的飞起；生成任务，目前人工智障 NLP 能力的天花板，虽然经常会处在说不出来人话的状态，但也不断吸引 CopyNet、VAE、GAN 各类选手前来挑战；唯有序列标注，数年如一日，不忘初心，原地踏步，到现在一提到 NER，还是会一下子只想到 LSTM-CRF，铁打不动的模型，没得挑也不用挑，用就完事了，不用就是不给面子

ACL2020 做NER的论文主要在做这几个方面：

> 1. 多特征：实体识别不是一个特别复杂的任务，不需要太深入的模型，那么就是加特征，特征越多效果越好，所以字特征、词特征、词性特征、句法特征、KG表征等等的就一个个加吧，甚至有些中文 NER 任务里还加入了拼音特征、笔画特征。。？心有多大，特征就有多多
> 2. 多任务：很多时候做 NER 的目的并不仅是为了 NER，而是服务于一个更大的目标或系统，比如信息抽取、问答系统等等。如果把整个大任务做一个端到端的模型，就需要做成一个多任务模型，把 NER 作为其中一个子任务；另外，单纯的 NER 也可以做成多任务，比如实体类型过多时，仅用一个序列标注任务来同时抽取实体与判断实体类型，会有些力不从心，就可以拆成两个子任务来做.
> 3. 时令大杂烩：把当下比较流行的深度学习话题或方法跟 NER 结合一下，比如结合强化学习的 NER、结合 few-shot learning 的 NER、结合多模态信息的 NER、结合跨语种学习的 NER 等等的，具体就不提了。

参考：[流水的NLP铁打的NER：命名实体识别实践与探索](https://zhuanlan.zhihu.com/p/166496466)

### 数据集

难搞哦~

## 2. 杂七杂八知识点

这里的知识点非常的零碎，以供日后需要时随时查看。

### 条件随机场 CRF

西瓜书上也有（还没来得及看）

上面那篇文章多次提到了CRF，这里就先了解一下什么是CRF。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210729125407-image.png)

参考资料：

1. [CRF原理及实现代码 - 微信](https://mp.weixin.qq.com/s/Ql1YGJvH68K8_PIctDsU-Q)

### N-Shot Learning

为了解决数据缺少的问题而提出的方法。

参考：[N-Shot Learning: Learning More with Less Data](https://blog.floydhub.com/n-shot-learning/)、[小样本学习（Few-shot Learning）综述  - 阿里云小蜜团队](https://zhuanlan.zhihu.com/p/61215293)

### 依存路径

1. 最短依存路径 shortest dependency path
2. 增广依存路径 augmented dependency path

比如这是Jingbo who dresses a green t-shirt was instructed by Chen.”的依存树形图
![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210730102258-image.png)

最短依存路径指的是找到 head 和 tail 之间的句法路径，就是找到一个节点，把自己和所有父节点放到一个数组里，再在另一个节点，从本身开始顺着父节点找，直到找到和第一个节点并且存在于第一个数组里，这样，第一个数组从 0 开始到这个公共节点和第二个节点的从这个节点到自己本身的所有节点就是这俩节点的最短路径。如在上图中“Jingbo“和”Chen“之间的最短依存路径为 Jingbo-instructed-by-Chen，长度为 3。

说人话就是找最近的公共父节点！

参考：[A survey on dependency-based technology of NLP](https://www.zybuluo.com/thousfeet/note/1418776)、[某PDF](https://hzaubionlp.files.wordpress.com/2020/09/3e38081e59fbae4ba8espacye5928cnetworkxe79a84e4be9de5ad98e6a091e5928ce69c80e79fade4be9de5ad98e8b7afe5be84e58886e69e90.pdf)、[关系抽取论文整理，核方法、远程监督的重点都在这里](https://mp.weixin.qq.com/s/glJbj9EkI67kyIBCZCHrkw)

### 远程监督

distantly supervised 简单来说，是利用外部资源（远程），如知识图谱数据通过对齐文本生成关系提取中所需要的带标签的数据，之后利用这些数据做有监督（监督）的训练。不过此方法一般使用在关系提取上面；

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210809221042-image.png)

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210809221410-image.png)

参考：[Distant supervision for relation extraction 远程监督](https://zhuanlan.zhihu.com/p/315450600)

### 真假 RNN

- recurrent neural network(循环神经网络)
- recursive neural network(递归神经网络)

循环神经网络是时间上的展开，处理的是序列结构的信息；递归神经网络是空间上的展开，处理的是树状结构的信息；

参考：[如何有效的区分和理解RNN循环神经网络与递归神经网络？ - 知乎](https://www.zhihu.com/question/36824148)

### 多模态

待学习

### 边缘概率

隐约觉得上课的时候学过（丢死个人）

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210801135302-image.png)

参考：[边缘分布 - 维基百科](https://zh.wikipedia.org/wiki/边缘分布)

### 耿贝尔分布 Gumbel

又是一个概率问题，百度百科[耿贝尔分布](https://baike.baidu.com/item/耿贝尔分布/1381013)

### NLP的颗粒度问题

此处参考了《数学之美》第四章 谈谈中文分词 2.2 词的颗粒度与层次

在不同任务中，颗粒度大小对效果的影响是不一样的，一般而言，颗粒度大翻译效果好（“联想公司” => Lenovo），对于网页搜索，颗粒度小效果好（“清华” => “清华大学”）；

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210809204515-image.png)

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210809204701-image.png)

### Lattice 什么意思

网格网络，我所知道的最早是由[Chinese NER Using Lattice LSTM](https://arxiv.org/pdf/1805.02023.pdf)所提出的，对于中文任务而言，天然并不存在像英文单词一样由空格组成的词语分界；而如果利用一些中文分词的工具对中文数据进行分词的话，又会有很多的错误。所以采用中文字符级信息和分词的信息同时传入网络会有更好的效果，具体而言参照这个网络。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210809201916-image.png)

而对于这两个特征的融合的办法，可以参见这里的解读：

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210809202012-image.png)

参考：[Lattice LSTM解读](https://zhuanlan.zhihu.com/p/348152603)

### 可学习的位置编码

在 ChineseBert 里面看到的，“The input to the model is the addition of the **learnable absolute positional embedding** and the fusion embedding……”，我记得 Transformer 里面的位置编码是固定的吧；

查了下资料，原来 Bert 的结构就是使用的可学习的位置编码嵌入，主要原因可能如下：

1. 两者精度差别不大；
2. Bert 的部分任务并不关注位置的顺序；

参考：[知乎 - BERT为何使用学习的position embedding而非正弦position encoding?](https://www.zhihu.com/question/307293465/answer/712178635)

## 3. 论文笔记

### MECT

MECT: Multi-Metadata Embedding based Cross-Transformer for Chinese Named Entity Recognition（arXiv:2107.05418v1  [cs.CL]  12 Jul 2021）

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210721190521-image.png)

主要贡献：

1. 在中文NER使用汉字的多元数据特征嵌入。
2. 提出了一种新的双流模型，该模型结合了汉字的部首、字符和单词，提高了所提方法的性能。
3. 在几个著名的中文NER基准数据集上对所提出的方法进行了评估，证明了所提出的方法优于最先进的方法。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210721190840-image.png)

参考：[CNNcharEmbedding](https://github.com/fastnlp/fastNLP/blob/master/fastNLP/embeddings/char_embedding.py)

## 4. 关系提取综述

[[2004.03186] More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction (arxiv.org)](https://arxiv.org/abs/2004.03186)

[关系提取综述笔记](https://shimo.im/mindmaps/d9CJx8CGD9hp9CYd/)

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210729195538-关系提取综述笔记.jpeg)
