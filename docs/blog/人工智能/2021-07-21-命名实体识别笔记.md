---
title: 命名实体识别笔记
date: 2021-07-21 13:02:51
permalink: /ner-beginner-note/
cover: 
tags: 
- 人工智能
- NLP
categories: 人工智能

---

## 1. 命名实体识别 NER

参考资料：

[流水的NLP铁打的NER：命名实体识别实践与探索 - 王岳王院长的文章 - 知乎](https://zhuanlan.zhihu.com/p/166496466)


一篇很好的综述：[[1812.09449] A Survey on Deep Learning for Named Entity Recognition (arxiv.org)](https://arxiv.org/abs/1812.09449)

这是我写的思维导图笔记：[《命名实体识别》](https://shimo.im/mindmaps/3PCHWJCYkw98cgvR/)

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210728213903-命名实体识别.jpeg)

NLP四大任务

> 为什么说流水的NLP铁打的NER？NLP四大任务嘛，分类、生成、序列标注、句子对标注。分类任务，面太广了，万物皆可分类，各种方法层出不穷；句子对标注，经常是体现人工智能（zhang）对人类语言理解能力的标准秤，孪生网络、DSSM、ESIM 各种模型一年年也是秀的飞起；生成任务，目前人工智障 NLP 能力的天花板，虽然经常会处在说不出来人话的状态，但也不断吸引 CopyNet、VAE、GAN 各类选手前来挑战；唯有序列标注，数年如一日，不忘初心，原地踏步，到现在一提到 NER，还是会一下子只想到 LSTM-CRF，铁打不动的模型，没得挑也不用挑，用就完事了，不用就是不给面子

ACL2020 做NER的论文主要在做这几个方面：

> 1. 多特征：实体识别不是一个特别复杂的任务，不需要太深入的模型，那么就是加特征，特征越多效果越好，所以字特征、词特征、词性特征、句法特征、KG表征等等的就一个个加吧，甚至有些中文 NER 任务里还加入了拼音特征、笔画特征。。？心有多大，特征就有多多
> 2. 多任务：很多时候做 NER 的目的并不仅是为了 NER，而是服务于一个更大的目标或系统，比如信息抽取、问答系统等等。如果把整个大任务做一个端到端的模型，就需要做成一个多任务模型，把 NER 作为其中一个子任务；另外，单纯的 NER 也可以做成多任务，比如实体类型过多时，仅用一个序列标注任务来同时抽取实体与判断实体类型，会有些力不从心，就可以拆成两个子任务来做.
> 3. 时令大杂烩：把当下比较流行的深度学习话题或方法跟 NER 结合一下，比如结合强化学习的 NER、结合 few-shot learning 的 NER、结合多模态信息的 NER、结合跨语种学习的 NER 等等的，具体就不提了。


## 2. 杂七杂八知识点

### 条件随机场 CRF

上面那篇文章多次提到了CRF，这里就先了解一下什么是CRF。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210729125407-image.png)

参考资料：
1. [CRF原理及实现代码 - 微信](https://mp.weixin.qq.com/s/Ql1YGJvH68K8_PIctDsU-Q)

### N-Shot Learning

为了解决数据缺少的问题而提出的方法。

参考：[N-Shot Learning: Learning More with Less Data](https://blog.floydhub.com/n-shot-learning/)、[小样本学习（Few-shot Learning）综述  - 阿里云小蜜团队](https://zhuanlan.zhihu.com/p/61215293)

### 依存路径

1. 最短依存路径 shortest dependency path
2. 增广依存路径 augmented dependency path

比如这是Jingbo who dresses a green t-shirt was instructed by Chen.”的依存树形图
![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210730102258-image.png)

最短依存路径指的是找到 head 和 tail 之间的句法路径，就是找到一个节点，把自己和所有父节点放到一个数组里，再在另一个节点，从本身开始顺着父节点找，直到找到和第一个节点并且存在于第一个数组里，这样，第一个数组从 0 开始到这个公共节点和第二个节点的从这个节点到自己本身的所有节点就是这俩节点的最短路径。如在上图中“Jingbo“和”Chen“之间的最短依存路径为 Jingbo-instructed-by-Chen，长度为 3。

说人话就是找最近的公共父节点！

参考：[A survey on dependency-based technology of NLP](https://www.zybuluo.com/thousfeet/note/1418776)、[某PDF](https://hzaubionlp.files.wordpress.com/2020/09/3e38081e59fbae4ba8espacye5928cnetworkxe79a84e4be9de5ad98e6a091e5928ce69c80e79fade4be9de5ad98e8b7afe5be84e58886e69e90.pdf)、[关系抽取论文整理，核方法、远程监督的重点都在这里](https://mp.weixin.qq.com/s/glJbj9EkI67kyIBCZCHrkw)

### 远程监督 distantly supervised

待学习

### RNN

- recurrent neural network(循环神经网络)
- recursive neural network(递归神经网络)

循环神经网络是时间上的展开，处理的是序列结构的信息；递归神经网络是空间上的展开，处理的是树状结构的信息；

参考：[如何有效的区分和理解RNN循环神经网络与递归神经网络？ - 知乎](https://www.zhihu.com/question/36824148)



## 3. 论文笔记

### MECT

MECT: Multi-Metadata Embedding based Cross-Transformer for Chinese Named Entity Recognition（arXiv:2107.05418v1  [cs.CL]  12 Jul 2021）

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210721190521-image.png)

主要贡献：

1. 在中文NER使用汉字的多元数据特征嵌入。
2. 提出了一种新的双流模型，该模型结合了汉字的部首、字符和单词，提高了所提方法的性能。
3. 在几个著名的中文NER基准数据集上对所提出的方法进行了评估，证明了所提出的方法优于最先进的方法。


![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210721190840-image.png)

## 4. 关系提取综述

[[2004.03186] More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction (arxiv.org)](https://arxiv.org/abs/2004.03186)

[关系提取综述笔记](https://shimo.im/mindmaps/d9CJx8CGD9hp9CYd/)

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210729195538-关系提取综述笔记.jpeg)

