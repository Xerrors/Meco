---
title: 实体关系提取论文阅读笔记补充篇
date: 2021-08-25 14:42:07
permalink: /ere-note-expend-1/
cover: 
tags: 
- NLP
- ERE
categories: NLP
---
这篇文章主要是记录自己在阅读实体关系提取的相关论文的简单笔记

## 1. 介绍

论文列表：

- [A Relation-Specific Attention Network for Joint Entity and Relation Extraction (ijcai.org)](https://www.ijcai.org/proceedings/2020/0561.pdf)

## 2. 详细记录

这部分会仔细介绍论文的动机以及模型可能存在的问题。

### 基于关系的注意力网络

论文在此：[A Relation-Specific Attention Network for Joint Entity and Relation Extraction (ijcai.org)](https://www.ijcai.org/proceedings/2020/0561.pdf)

先看摘要：

> 实体和关系的联合抽取是自然语言处理(NLP)中的一项重要任务，其目的是从纯文本中获取所有的关系三元组。这是一个很大的挑战，因为从一个句子中提取的一些三元组可能有重叠的实体。现有的大多数方法都是先进行实体识别，然后再检测每个可能的实体对之间的关系，这通常需要进行大量的冗余操作。本文提出了一种**基于关系的注意力网络(RSAN)**来解决这一问题。我们的RSAN利用**关系感知的注意机制**为每个关系构建特定的句子表示，然后进行**序列标注**以提取其对应的头部和尾部实体。在两个公开数据集上的实验表明，我们的模型能够有效地提取重叠的三元组，并取得了最好的性能。我们的代码可以在[github.com/Anery/RSAN](https://github.com/Anery/RSAN)上找到

作者的核心假设在于：**在不同的关系下，词语对句子的底层语义表达应该有不同的贡献**。（虽然这跟后续 PURE 所做的实验并不一致，在此先可以这么认为）

#### 模型介绍

从摘要的加粗部分可以看到，此文作者所提出的模型依然是采用序列标注的方法来预测实体的；但是为了减少冗余，作者是先预测了句子中可能存在的关系，将问题转化为多序列标注问题；

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210825190045-imagepng)

下图是整个模型的结构图，图中是以一个关系为例来介绍模型的运算过程。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210825190206-imagepng)

我大致将整个过程分为：句子编码、基于关系的注意力机制、关系门机制、特定关系的实体解码器

**句子编码**

句子编码的输入包括了词嵌入、part-of-speech(POS)以及字符级特征；通过 BiLSTM 之后就可以得到包含上下文信息的特征向量 $\mathbf{h}_i = \operatorname{LSTM}(\mathbf{x}_i)$；

**基于关系对句子加权**

由于核心假设认为**在不同的关系下，词语对句子的底层语义表达应该有不同的贡献**。所以需要针对不同的关系对句子中的单词进行注意力加权，注意力加权的方式如下公式：

$$
\begin{aligned}
\mathbf{s}_{g} &=\operatorname{avg}\left\{\mathbf{h}_{1}, \mathbf{h}_{2}, \ldots, \mathbf{h}_{n}\right\} \\
\mathbf{e}_{i k} &=\mathbf{v}^{T} \tanh \left(\mathbf{W}_{r} \mathbf{r}_{k}+\mathbf{W}_{g} \mathbf{s}_{g}+\mathbf{W}_{h} \mathbf{h}_{i}\right) \\
\alpha_{i k} &=\frac{\exp \left(\mathbf{e}_{i k}\right)}{\sum_{j=1}^{n} \exp \left(\mathbf{e}_{j k}\right)}
\end{aligned}

$$

其中 $\mathbf{s}_g$ 表示句子的全局表征，$\mathbf{r}\in \mathbb{R}^{d_r}$ 是个可学习的嵌入矩阵， $\mathbf{r}_k$ 表示第 $k$ 个关系；$\alpha_{i k}$ 就表示在关系 $k$ 下，第 $i$ 个单词的注意力权重，对句子进行加权求和之后也就得到了**特定的句子表征**。$\mathbf{s}_{k} = \sum_{i=1}^{n} \alpha_{i k} \mathbf{h}_{i}$

**利用关系门机制得到单词的最终表征**

这里需要注意，只有当关系对句子是正向效果的时候才会使用上述关系加权去预测实体，否则会影响后续的实体解码过程；

$$
\begin{aligned}
g_{k} &=\sigma\left(\left(\mathbf{W}_{1} \mathbf{s}_{g}+b_{1}\right) \oplus\left(\mathbf{W}_{2} \mathbf{s}_{k}+b_{2}\right)\right) \\
\mathbf{u}_{\mathbf{k}} &=g_{k} \odot \tanh \left(\mathbf{W}_{3} \mathbf{s}_{k}+b_{3}\right)
\end{aligned}

$$

其中 $g_k$ 是用来衡量原有的句子表示 $\mathbf{s}_g$ 和基于关系的表示 $\mathbf{s}_k$ 哪一个更有利于实体提取。$\mathbf{u_k}$ 则是表示保留下来的关系特征。那么最终的单词表征就是将两者拼接起来：$\mathbf{h}_i^k = \mathbf{h}_i \oplus \mathbf{u_k} $；

不过我有个疑问，这岂不是针对每个关系都去预测实体，如果关系变得很多的话，其实不是很麻烦，应该先过滤一下不太可能存在于这个句子中的关系吧。

**实体解码**
