---
title: 那些年我们入门菜鸡看不懂的术语
date: 2021-08-16 19:52:41
permalink: /terminologies-that-vegbird-can-not-understand/
cover: https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210816203007-imagepng
tags: 
- 笔记
- NLP
categories: NLP
---
因为是刚刚入门自然语言处理，这方面的基础可以说是非常的薄弱了，以至于很多时候并不知道这些术语说的都是什么意思，这里就做一个记录，等有时间的时候再去找资料查文献了解一下，怕的不是不会，而是第一次遇见不会，第十次遇见还是不会！共勉！

## 1. 记录

### 匈牙利算法

wiki: [匈牙利算法 - 维基百科，自由的百科全书 (wikipedia.org)](https://zh.wikipedia.org/wiki/%E5%8C%88%E7%89%99%E5%88%A9%E7%AE%97%E6%B3%95)

### 自回归是什么？

[[2011.01675] Joint Entity and Relation Extraction with Set Prediction Networks (arxiv.org)](https://arxiv.org/abs/2011.01675)

这里面提到的自回归网络和非自回归网络

### 负样本学习

是指对比学习吗？是怎么学习的呢？难不成比较他们之间的 loss 然后减掉吗？

### Partial Match 与 Exact Match

表示在关系提取的实验中所采用的匹配标准，即部分匹配和精确匹配：

- 部分匹配：如果关系正确并且两个实体的 head 都是正确的，则认为三元组是正确的；
- 精准匹配：需要关系分类正确且两个实体的边界（span）是正确的才可以；

### 结构化预测与关系重叠问题

联合抽取方法的一种方式，即统一为全局优化问题进行联合解码，只需要一个阶段解码，解决暴漏偏差。除此之外还有**多任务学习** ：即实体和关系任务共享同一个编码器，但通常会依赖先后的抽取顺序：关系判别通常需要依赖实体抽取结果。这种方式会存在暴漏偏差，会导致误差积累。

结构化预测最早出现在17年论文《Joint extraction of entities and relations based on a novel tagging scheme》中。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210817145031-imagepng)

这篇论文用一个统一的序列标注框架抽取实体关系，如上图所示：直接以关系标签进行BIOES标注，subject实体序号为1，object实体序号为2。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210817145006-imagepng)

不过，以关系标签进行BIOES标注这种方式不能解决**关系重叠**问题，如上图所示：

1. **SEO** ：SingleEntityOverlap，一个实体出现在多个关系三元组中；
2. **EPO** ：EntityPairOverlap，一个实体pair有多种关系；

也就是说： **结构化预测不能在解决暴漏偏差的同时，却不能cover关系重叠问题** 。因此，TPLinker要同时能够解决这两个问题。

参考：[实体关系抽取新范式！TPLinker：单阶段联合抽取，并解决暴漏偏差～ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/346897151)

### 实体pair的pooling，Mention Pooling

mark

### One-Pass 与 Multi-Pass

mark

### 标签依赖问题是什么

pass

### 暴露偏差问题

指在训练阶段是gold实体输入进行关系预测，而在推断阶段是上一步的预测实体输入进行关系判断；导致训练和推断存在不一致。[实体关系抽取新范式！TPLinker：单阶段联合抽取，并解决暴漏偏差～ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/346897151)

大多数 NMT 模型都基于编码器-解码器框架，这些模型基于之前的文本来预测下一个词，得到目标词的语言模型。在训练阶段，将真实词（ground truth word）用作上下文（context）输入，而在推理时，由于整个序列由得到的模型自行生成，所以将模型生成的前一个词用作上下文输入。因此，训练和推理时的预测词是从不同的分布中提取出来的：训练时的预测词是从数据分布中提取的，而推理时的预测词是从模型分布中提取的。这种差异称为 **暴露偏差** ，导致了训练和推理之间的差距。随着目标序列的增长，误差会随之累积，模型必须在训练时从未遇到的情况下进行预测。[中科院、华为等斩获ACL最佳长论文：如何弥合神经机器翻译在训练和推理之间的缺口？-InfoQ](https://www.infoq.cn/article/ivb8auo_ty7xbbxvl66f)

### 条件随机场 CRF

西瓜书上也有（还没来得及看）

上面那篇文章多次提到了CRF，这里就先了解一下什么是CRF。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210729125407-image.png)

参考资料：

1. [CRF原理及实现代码 - 微信](https://mp.weixin.qq.com/s/Ql1YGJvH68K8_PIctDsU-Q)

### N-Shot Learning

为了解决数据缺少的问题而提出的方法。

参考：[N-Shot Learning: Learning More with Less Data](https://blog.floydhub.com/n-shot-learning/)、[小样本学习（Few-shot Learning）综述  - 阿里云小蜜团队](https://zhuanlan.zhihu.com/p/61215293)

### 依存路径

1. 最短依存路径 shortest dependency path
2. 增广依存路径 augmented dependency path

比如这是Jingbo who dresses a green t-shirt was instructed by Chen.”的依存树形图
![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210816203007-imagepng)

最短依存路径指的是找到 head 和 tail 之间的句法路径，就是找到一个节点，把自己和所有父节点放到一个数组里，再在另一个节点，从本身开始顺着父节点找，直到找到和第一个节点并且存在于第一个数组里，这样，第一个数组从 0 开始到这个公共节点和第二个节点的从这个节点到自己本身的所有节点就是这俩节点的最短路径。如在上图中“Jingbo“和”Chen“之间的最短依存路径为 Jingbo-instructed-by-Chen，长度为 3。

说人话就是找最近的公共父节点！

参考：[A survey on dependency-based technology of NLP](https://www.zybuluo.com/thousfeet/note/1418776)、[某PDF](https://hzaubionlp.files.wordpress.com/2020/09/3e38081e59fbae4ba8espacye5928cnetworkxe79a84e4be9de5ad98e6a091e5928ce69c80e79fade4be9de5ad98e8b7afe5be84e58886e69e90.pdf)、[关系抽取论文整理，核方法、远程监督的重点都在这里](https://mp.weixin.qq.com/s/glJbj9EkI67kyIBCZCHrkw)

### 远程监督

distantly supervised 简单来说，是利用外部资源（远程），如知识图谱数据通过对齐文本生成关系提取中所需要的带标签的数据，之后利用这些数据做有监督（监督）的训练。不过此方法一般使用在关系提取上面；

参考：[Distant supervision for relation extraction 远程监督](https://zhuanlan.zhihu.com/p/315450600)

### 真假 RNN

经常遇到的有两个 RNN

- recurrent neural network(循环神经网络)
- recursive neural network(递归神经网络)

循环神经网络是时间上的展开，处理的是序列结构的信息；递归神经网络是空间上的展开，处理的是树状结构的信息；

参考：[如何有效的区分和理解RNN循环神经网络与递归神经网络？ - 知乎](https://www.zhihu.com/question/36824148)

### 边缘概率

隐约觉得上课的时候学过（丢死个人）

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210801135302-image.png)

参考：[边缘分布 - 维基百科](https://zh.wikipedia.org/wiki/边缘分布)

### 耿贝尔分布 Gumbel

又是一个概率问题，百度百科[耿贝尔分布](https://baike.baidu.com/item/耿贝尔分布/1381013)

### NLP的颗粒度问题

此处参考了《数学之美》第四章 谈谈中文分词 2.2 词的颗粒度与层次；颗粒度就是指中文分词的时候的精细程度。

在不同任务中，颗粒度大小对效果的影响是不一样的，一般而言，颗粒度大翻译效果好（“联想公司” => Lenovo），对于网页搜索，颗粒度小效果好（“清华” => “清华大学”）；

### Lattice 什么意思

网格网络，我所知道的最早是由[Chinese NER Using Lattice LSTM](https://arxiv.org/pdf/1805.02023.pdf)所提出的，对于中文任务而言，天然并不存在像英文单词一样由空格组成的词语分界；而如果利用一些中文分词的工具对中文数据进行分词的话，又会有很多的错误。所以采用中文字符级信息和分词的信息同时传入网络会有更好的效果，具体而言参照这个网络。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210809201916-image.png)

而对于这两个特征的融合的办法，可以参见这里的解读：[图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210809202012-image.png)

参考：[Lattice LSTM解读](https://zhuanlan.zhihu.com/p/348152603)

### 可学习的位置编码

在 ChineseBert 里面看到的，“The input to the model is the addition of the **learnable absolute positional embedding** and the fusion embedding……”，我记得 Transformer 里面的位置编码是固定的吧；

查了下资料，原来 Bert 的结构就是使用的可学习的位置编码嵌入，主要原因可能如下：

1. 两者精度差别不大；
2. Bert 的部分任务并不关注位置的顺序；

参考：[知乎 - BERT为何使用学习的position embedding而非正弦position encoding?](https://www.zhihu.com/question/307293465/answer/712178635)

## 总结

学无止境啊，你看我二十多岁还这么精神，就是早上起来头皮发凉。
