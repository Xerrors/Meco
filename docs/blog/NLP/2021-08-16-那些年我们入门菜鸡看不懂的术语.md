---
title: 那些年我们入门菜鸡看不懂的术语
date: 2021-08-16 19:52:41
permalink: /terminologies-that-vegbird-can-not-understand/
cover: 
tags: 
- 笔记
- NLP
categories: NLP
---
因为是刚刚入门自然语言处理，这方面的基础可以说是非常的薄弱了，以至于很多时候并不知道这些术语说的都是什么意思，这里就做一个记录，等有时间的时候再去找资料查文献了解一下，怕的不是不会，而是第一次遇见不会，第十次遇见还是不会！共勉！

## 1. 记录

### 条件随机场 CRF

西瓜书上也有（还没来得及看）

上面那篇文章多次提到了CRF，这里就先了解一下什么是CRF。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210729125407-image.png)

参考资料：

1. [CRF原理及实现代码 - 微信](https://mp.weixin.qq.com/s/Ql1YGJvH68K8_PIctDsU-Q)

### N-Shot Learning

为了解决数据缺少的问题而提出的方法。

参考：[N-Shot Learning: Learning More with Less Data](https://blog.floydhub.com/n-shot-learning/)、[小样本学习（Few-shot Learning）综述  - 阿里云小蜜团队](https://zhuanlan.zhihu.com/p/61215293)

### 依存路径

1. 最短依存路径 shortest dependency path
2. 增广依存路径 augmented dependency path

比如这是Jingbo who dresses a green t-shirt was instructed by Chen.”的依存树形图
![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210730102258-image.png)

最短依存路径指的是找到 head 和 tail 之间的句法路径，就是找到一个节点，把自己和所有父节点放到一个数组里，再在另一个节点，从本身开始顺着父节点找，直到找到和第一个节点并且存在于第一个数组里，这样，第一个数组从 0 开始到这个公共节点和第二个节点的从这个节点到自己本身的所有节点就是这俩节点的最短路径。如在上图中“Jingbo“和”Chen“之间的最短依存路径为 Jingbo-instructed-by-Chen，长度为 3。

说人话就是找最近的公共父节点！

参考：[A survey on dependency-based technology of NLP](https://www.zybuluo.com/thousfeet/note/1418776)、[某PDF](https://hzaubionlp.files.wordpress.com/2020/09/3e38081e59fbae4ba8espacye5928cnetworkxe79a84e4be9de5ad98e6a091e5928ce69c80e79fade4be9de5ad98e8b7afe5be84e58886e69e90.pdf)、[关系抽取论文整理，核方法、远程监督的重点都在这里](https://mp.weixin.qq.com/s/glJbj9EkI67kyIBCZCHrkw)

### 远程监督

distantly supervised 简单来说，是利用外部资源（远程），如知识图谱数据通过对齐文本生成关系提取中所需要的带标签的数据，之后利用这些数据做有监督（监督）的训练。不过此方法一般使用在关系提取上面；

参考：[Distant supervision for relation extraction 远程监督](https://zhuanlan.zhihu.com/p/315450600)

### 真假 RNN

经常遇到的有两个 RNN

- recurrent neural network(循环神经网络)
- recursive neural network(递归神经网络)

循环神经网络是时间上的展开，处理的是序列结构的信息；递归神经网络是空间上的展开，处理的是树状结构的信息；

参考：[如何有效的区分和理解RNN循环神经网络与递归神经网络？ - 知乎](https://www.zhihu.com/question/36824148)

### 边缘概率

隐约觉得上课的时候学过（丢死个人）

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210801135302-image.png)

参考：[边缘分布 - 维基百科](https://zh.wikipedia.org/wiki/边缘分布)

### 耿贝尔分布 Gumbel

又是一个概率问题，百度百科[耿贝尔分布](https://baike.baidu.com/item/耿贝尔分布/1381013)

### NLP的颗粒度问题

此处参考了《数学之美》第四章 谈谈中文分词 2.2 词的颗粒度与层次；颗粒度就是指中文分词的时候的精细程度。

在不同任务中，颗粒度大小对效果的影响是不一样的，一般而言，颗粒度大翻译效果好（“联想公司” => Lenovo），对于网页搜索，颗粒度小效果好（“清华” => “清华大学”）；

### Lattice 什么意思

网格网络，我所知道的最早是由[Chinese NER Using Lattice LSTM](https://arxiv.org/pdf/1805.02023.pdf)所提出的，对于中文任务而言，天然并不存在像英文单词一样由空格组成的词语分界；而如果利用一些中文分词的工具对中文数据进行分词的话，又会有很多的错误。所以采用中文字符级信息和分词的信息同时传入网络会有更好的效果，具体而言参照这个网络。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210809201916-image.png)

而对于这两个特征的融合的办法，可以参见这里的解读：[图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210809202012-image.png)

参考：[Lattice LSTM解读](https://zhuanlan.zhihu.com/p/348152603)

### 可学习的位置编码

在 ChineseBert 里面看到的，“The input to the model is the addition of the **learnable absolute positional embedding** and the fusion embedding……”，我记得 Transformer 里面的位置编码是固定的吧；

查了下资料，原来 Bert 的结构就是使用的可学习的位置编码嵌入，主要原因可能如下：

1. 两者精度差别不大；
2. Bert 的部分任务并不关注位置的顺序；

参考：[知乎 - BERT为何使用学习的position embedding而非正弦position encoding?](https://www.zhihu.com/question/307293465/answer/712178635)

## 总结

学无止境啊，你看我二十多岁还这么精神，就是早上起来头皮发凉。
