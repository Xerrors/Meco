---
title: 近期论文小结（多模态协同训练）
date: 2021-08-11 21:57:05
permalink: /2021-08-11-week-post/
cover: https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210430165756-image.png
tags: 
- 周报
categories: 周报
---
## 1. 多模态预训练方法

### ViLBERT

这个了解不多，首先是看到了这个论文：面向视觉基础进行预训练！（第一次看到这么调皮的标题）[[1908.02265] ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks (arxiv.org)](https://arxiv.org/abs/1908.02265)

![ViLBERT](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210811220542.png)

这里是解读文章：[BERT新转变：面向视觉基础进行预训练！ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/101511981)，很不错，原文实在是晦涩难懂！

![](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210811220755.png)

![](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210811220822.png)

![](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210811220842.png)

### UNIMO

解读文章：[机器之心](https://mp.weixin.qq.com/s/CUR57R-4xXHpVLFCJTG55w)，原谅我看不懂原文！

![](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210811221154.png)

> **百度提出面向异构模态数据的统一预训练方法 UNIMO**，在具体训练过程中，文本、图像和图文对**三种模态数据随机混合**在一起，其中图像被转换为目标（object）序列，文本被转换为词（token）序列，图文对被转换为目标序列和词序列的拼接。
>
> UNIMO 对三种类型数据进行统一处理，在目标序列或者词序列上基于掩码预测进行**自监督学习**，并且基于图文对数据进行**跨模态对比学习**，从而实现图像与文本的统一表示学习。进一步的，这种联合学习方法也让文本知识和视觉知识互相增强，从而有效提升文本语义表示和视觉语义表示的能力。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210811222953-_20210811222737jpg)

> 异构模态的统一预训练最大的挑战是如何跨越不同模态间的语义鸿沟从而实现语义表示的统一。如下图所示，UNIMO 提出了创新的**跨模态对比学习**方法，同时引入相关联的图文对数据、文本数据和图像数据进行**联合对比学习**。具体地，UNIMO 通过文本改写的方式，对图文对进行数据增广，获得大量的正例和强负例图文对数据。同时为了更好的利用文本和图像数据，UNIMO 通过文本与图像检索，获得相关的图像和文本作为正例。这样利用扩充后的多种类型的正例以及高质量强负例，**UNIMO 在统一的语义空间上进行联想对比，从而能够学习到精确对齐的跨模态语义表示**。注：玄学！

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210811223408-_20210811222745jpg)

下面一起来看看实验是怎么做的，反正就是很多很多数据。

> 在实验方面，UNIMO 使用了大量的文本、图像和图文数据进行联合学习，同时在各种单一模态和跨模态下游任务上进行验证。预训练数据部分，**文本语料**包括 Wikipedia、BookCorpus、OpenWebText 等共 54G 语料；**图像数据**是从互联网爬取的 170 万张图像；而**图文对数据**则包括 COCO Caption、Visual Genome、Conceptual Caption、SBU Caption。
>
> 下游任务既包括图文搜索、视觉问答、图描述生成、视觉推断等跨模态任务，也包括文本分类、阅读理解、文本摘要、问题生成等各种文本任务。模型上，Base 基于 12 层的 Transformer，而 Large 使用 24 层。

看也看不懂，只能喊666了
