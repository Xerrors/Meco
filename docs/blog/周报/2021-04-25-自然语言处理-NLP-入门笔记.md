---
title: 自然语言处理 NLP 入门笔记
date: 2021-04-25 11:12:31
permalink: /week-post-04-25/
cover: https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210820175248-imagepng
tags: 
- 人工智能
categories: 周报
abstract: 最近开始学习自然语言处理相关的内容，作为入门小白，把自己的学习流程记录一下。同时也记录了一些学习中忘记了的一些简单的操作，方便之后查询使用。
---
最近开始学习自然语言处理相关的内容，作为入门小白，把自己的学习流程记录一下。

## 1. 吴恩达课程

第一阶段是先把吴恩达的网课稍微看了一下，整理出来了一些笔记，记录在[石墨文档](https://shimo.im/mindmaps/Q9dkGpwXKWyY6xYt/)里面了，写的比较简单，方便之后查看。

整个课程里面印象比较深的就是关于文本编码使用，也就是介绍了如何把自然语言编码成机器可以识别并理解的内容。同时网课也对 RNN、GRU、LSTM 和 seq2seq 做了简单的介绍，对于入门还是比较有帮助的。

## 2. BERT

吴恩达老师已经介绍了之前的一些网络模型，但是随着基础的发展，也涌现出一些新起之秀，接下来就是对这些新模型的学习。

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

看文章的时候发现一个大佬：[Jay Alammar](http://jalammar.github.io)，他写了很多关于自然语言处理相关的图示教程，简单易懂，很多其他的教程里面的插图都是从这里来的，之前是看 [Transformer](http://jalammar.github.io/illustrated-transformer/) 了解到的，这次看 [BERT](http://jalammar.github.io/illustrated-bert/) 同样还是有他，就连之前学习 [pandas](http://jalammar.github.io/gentle-visual-intro-to-data-analysis-python-pandas/) 的时候也是看他的文章理解的，更牛的是，他的文章还提供了很多其他语言的翻译版本（其他人翻译的），比如 [BERT 中文](https://blog.csdn.net/qq_41664845/article/details/84787969)，知识共享，向大佬致敬。

PS 最近上网冲浪的时候又发现了一个比较好的文章，[是有代码的 Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)，里面比较详细的介绍了如果部署 Transformer;

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210809214539-image.png)

文中提到了一些 NLP 领域的顶尖思想，我几乎没怎么听过（羞），这里记录一下：[Semi-supervised Sequence Learning](https://arxiv.org/abs/1511.01432) (by [Andrew Dai](https://twitter.com/iamandrewdai) and[ Quoc Le](https://twitter.com/quocleix)), [ELMo(这个我知道，用于词嵌入的)](https://arxiv.org/abs/1802.05365) (by [Matthew Peters](https://twitter.com/mattthemathman) and researchers from [AI2](https://allenai.org/) and [UW CSE](https://www.engr.washington.edu/about/bldgs/cse)), [ULMFiT](https://arxiv.org/abs/1801.06146) (by fast.ai founder [Jeremy Howard](https://twitter.com/jeremyphoward) and [Sebastian Ruder](https://twitter.com/seb_ruder)), and the [OpenAI transformer](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) (by OpenAI researchers [Radford](https://twitter.com/alecrad), [Narasimhan](https://twitter.com/karthik_r_n), [Salimans](https://twitter.com/timsalimans), and [Sutskever](https://twitter.com/ilyasut)), and the Transformer ([Vaswani et al](https://arxiv.org/pdf/1706.03762.pdf))。

文中描述「BERT的基础集成单元是Transformer的Encoder」，BERT 的输入跟 Encoder 确实没有什么区别，只是将第一个输出作为分类的标准，将输出的信息放到一个分类器里面，来进行最后的分类，得到类别。模型的框架结构如下图所示，图源自 [Jay Alammar](http://jalammar.github.io)。

![图示](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/bert-output-vector.png)

![图示](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/bert-classifier.png)

文中还回顾了一下词嵌入部分的内容，关于词嵌入部分的内容，吴恩达老师已经讲的比较清楚了，我上面的思维导图里面也有一些总结。

### ELMo 语境问题

语境化的词嵌入模型。我的理解是，同一个词在不同的语境下面，词嵌入的结果应该是不一样的，甚至可以当作不同的词来看待，这个图可谓是解释了精髓，醍醐灌顶！

![非常简洁易懂的图示](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/elmo-embedding-robin-williams.png)

具体的训练方法和实现方法还不是很了解，现在的理解是使用了 bi-LSTM 来预测下一个词和上一个词的输入，不明觉厉，先放两张图留着之后慢慢消化，图源自 [Jay Alammar](http://jalammar.github.io)。

![图示](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/elmo-secret.png)

![图示](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/elmo-forward-backward-language-model-embedding.png)

![图示](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/elmo-embedding.png)

复习一下：[Bi-LSTM 双向长短期记忆网络](https://www.jiqizhixin.com/articles/2018-10-24-13)

### ULM-FiT：NLP领域应用迁移学习

ULM-FiT机制让模型的预训练参数得到更好的利用。所利用的参数不仅限于embeddings，也不仅限于语境embedding，ULM-FiT引入了Language Model和一个有效微调该Language Model来执行各种NLP任务的流程。这使得NLP任务也能像计算机视觉一样方便的使用迁移学习。PS：这里我依然没有看懂，不过先记在这里，有机会可以再详细了解，总之就是加入了迁移学习。

### Transformer 超越LSTM的结构

关于 Transformer 的一切，都可以在这里 [Transformer](http://jalammar.github.io/illustrated-transformer/) 看到。有意思的是作者对于 Transformer 的评价，因为我没看懂：

> The Encoder-Decoder structure of the transformer made it perfect for machine translation. But how would you use it for sentence classification? How would you use it to pre-train a language model that can be fine-tuned for other tasks (downstream tasks is what the field calls those supervised-learning tasks that utilize a pre-trained model or component).
>
> Transformer Encoding和Decoding的结构非常适合机器翻译，但是怎么利用他来做文本分类的任务呢？实际上你只用使用它来预训练可以针对其他任务微调的语言模型即可。（这个翻译我给100分）

**OpenAI Transformer 用于语言模型的Transformer解码器预训练** ：简单来说就是一个训练好的解码器来进行文本分类。PS：？？？还是不懂，我真是个菜鸡！

**Transfer Learning to Downstream Tasks** : 将预训练好的模型应用到下游任务，比如训练一个语言模型，之后使用 hidden state 做分类。图源自 [Jay Alammar](http://jalammar.github.io)。

![Description](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/openai-transformer-sentence-classification.png)

下面是使用迁移学习来处理不同 NLP 任务的例子（完全看不懂，需要大佬带），图源自 [Jay Alammar](http://jalammar.github.io)：

![Description](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/openai-input-transformations.png)

### BERT Again

有了上面前人的铺垫，BERT 的提出也就理所应当、顺理成章、把前浪拍死在沙滩上！BERT 跟前面的 OpenAI Transformer 不一样的是使用了 encoder，同时使用 masks 来把需要预测的词给挡住。

> 我是没看懂，这是作者给的解释：语言模型会根据前面单词来预测下一个单词，但是self-attention的注意力只会放在自己身上，那么这样100%预测到自己，毫无意义，所以用Mask，把需要预测的词给挡住。图源自 [Jay Alammar](http://jalammar.github.io)。

![Description](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/BERT-language-modeling-masked-lm.png)

实际应用上，随机 mask 了 15% 的输入、混合了一些其他的东西、有时随机替换一个单词，同时要求预测正确。（是基于什么背景需要进行 mask，进行 mask 又能够改善哪些问题？这些都是需要仔细介绍的吧）

> 原文是这么描述的：Finding the right task to train a Transformer stack of encoders is a complex hurdle that BERT resolves by adopting a “masked language model” concept from earlier literature (where it’s called a Cloze task).
>
> 我个人感觉有点防止过拟合的意思，但是究竟是什么原因？可能需要深入的了解上面提到的 「Cloze Task」吧。注：Cloze Task 翻译过来是「完形填空」。

一些参考链接：[BERT Pytorch](https://github.com/huggingface/pytorch-pretrained-BERT)、[BERT FineTuning with Cloud TPUs](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)

## 3. ERNIE

ERNIE: Enhanced Representation through Knowledge Integration：通过知识整合增强特征。GitHub 地址：[PaddlePaddle/ERNIE](https://github.com/PaddlePaddle/ERNIE)、官网：[wenxin.baidu.com](https://wenxin.baidu.com/)、预印本：[arXiv:1904.09223](https://arxiv.org/abs/1904.09223)、官方讲解：[一文读懂最强中文NLP预训练模型ERNIE](https://baijiahao.baidu.com/s?id=1648169054540877476)。

居然还有个中文名「文心」，挺有文艺气息的哈！顺带一提，清华团队也提出来了一个叫 ERNIE 模型（[ERNIE: Enhanced Language Representation with Informative Entities](https://arxiv.org/abs/1905.07129)），稍微比百度早发表了一段时间，但是热度和讨论度都没有百度的高。

### 思路来源以及提升方向

先看一下论文中是怎么说的？首先 ERNIE 的提出也是受到 BERT 的 mask 的启发，优化的方向在于不是随机屏蔽，而是包括实体级屏蔽（entity-level masking）和短语级屏蔽（phrase-level masking）。（PS：然而这就是我没看懂的地方）

百度团队认为，BERT 只是逐字建模，没有针对词法和语法结构的建模，而 ERNIE 通过对训练数据中的词法结构，语法结构，语义信息进行统一建模，极大地增强了通用语义表示能力，在多项任务中均取得了大幅度超越BERT的效果。

首先从下图中可以看到 BERT 和 ERNIE 的不同 mask 策略：

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210427101631-masking-strategy-between-bert-and-ernie.png)

我又看到一个好玩的事情，在我读「[PaddlePaddle官方帐号](https://baijiahao.baidu.com/s?id=1648169054540877476)」关于 ERNIE 的介绍的时候，里面用来一张图来表示 ERNIE 的结构，而这张图是前面提到的博主 [Jay Alammar](http://jalammar.github.io) 的图，对比一下这两个图片，这是不是也就说明 ERNIE 在结构上面跟 BERT 一模一样，哈哈哈哈哈，不过这张图片没有出现在论文里面，不然就是学术造假了，哈哈哈哈，此外两个模型在层 layers、hidden units、attention heads 的数量上面是一模一样的，当然这个有可能是为了性能比较所以这么设置的。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210427101543-bert-structure.png)

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210427101610-ernie-structure.png)

笑死我算了，看来百度对 [Jay Alammar](http://jalammar.github.io) 大佬非常认可啊，后面在介绍 ERNIE 的结构的时候大量引用了 [Transformer](http://jalammar.github.io/illustrated-transformer/) 中对 Transformer 讲解的插图；考虑到这篇文章是面向讲解性质的，为了更好的解释而引用别人成功的图示也并没有任何问题。不过这确定也说明了 ERNIE 的创新点并不是很多，并没有对原有的结构有什么实质上的调整。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210427101713-different-masking-level-of-a-sentence.png)

## 其他学习记录

**Python Pandas 的 shuffle**

[参考链接](https://blog.csdn.net/qq_22238533/article/details/70917102)

```python
# 使用 pandas
df.sample(frac=1)

df.sample(frac=1).reset_index(drop=True)

# 使用 sklearn
from sklearn.utils import shuffle
df = shuffle(df)

# 使用 numpy
df.iloc[np.random.permutation(len(df))]
```

**Pandas 重命名表头**

[参考链接](https://blog.csdn.net/chenKFKevin/article/details/72847622)

```python
# 修改列名a，b为A、B
df.columns = ['A','B']

# 只修改列名a为A
df.rename(columns={'a':'A'})
```

**创建一个 DataFrame 并向里面添加数据**

[参考链接](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html)

使用 append 的第一个参数可以是 DataFrame、Series、字典。

```python
df = pd.DataFrame(columns = ["text", "label", "name"])
df = df.append({"text": "hello", "label": 0, "name": "IronMan"}, ignore_index=True)
```

**Pandas 对数据进行排序**

[参考链接](https://www.pypandas.cn/docs/user_guide/categorical.html#multi-column-sorting)

```python
df.sort_values(by=['A', 'B'])
```

**Pandas 格式化显示小数**

更多设置选项参考：[参考链接](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.set_option.html)，或者直接在 Jupyter 里面输入 `pd.set_option?`

```python
pd.set_option('display.float_format', lambda x:'%0.2f' % x)
```

**Pandas 分层采样**

好家伙，我找了半天才知道是这个名字「分层采样」，我试了「分组采样」、「聚合采样」、「聚类采样」。

```python
# 从每个 label 下面取 100 个样本
df.groupby('label').apply(lambda x: x.sample(n=100))
```

**Unicode 和 UTF-8 的关系**

参考链接：[Unicode 和 UTF-8 有什么区别？ - 盛世唐朝的回答 - 知乎](https://www.zhihu.com/question/23374078/answer/69732605)

**Python2 使用 UTF-8 读取文件**

[参考链接](https://blog.csdn.net/zkq_1986/article/details/81908785)

都 2021 年了，为啥还要用 Python2，Python3 不是已经支持了吗？都是泪啊！要是能用 Python3，谁用这 Python2 啊~

```python
import codecs
file = codecs.open("lol.txt","w",encoding="utf-8")
```

**Bi-gram**

[参考链接1](https://dev.to/amananandrai/language-model-implementation-bigram-model-22ij) 、[参考链接2](https://zhuanlan.zhihu.com/p/32829048)；还没有详细了解

简单的使用方法就在分词之后，把相邻的两两组合在一起，之后将组合后的跟之前的分词放在一起，作为语言模型的输入，也就是 bi-gram 了。同时还有 Tri-gram，相邻的三个组合在一起，这样可以提取到更多相邻词语的信息。

**Pandas 筛选某一列中最长的文本的长度**

[参考链接](https://cloud.tencent.com/developer/ask/134207)

```python
df_short.text.str.len().max()
```

## 近期简读

人工智能：

### 对比学习（Contrastive Learning）:研究进展精要

[链接](https://mp.weixin.qq.com/s/zNOymBLuMRO3pnMCMyiykw)

#### 提出的背景是什么？

1. 监督式学习的数据量有限，CV 领域预训练模型很难在下游获得很好的效果。
2. Bert 在 NLP 领域取得了无监督学习的成功，并且可以很好的应用到下游领域。

> 那对比学习是要干什么呢？从目标来说，对比学习就是要干NLP领域类似Bert预训练的事情

> 总体而言，图像领域里的自监督可以分为两种类型：生成式自监督学习(VAE,GAN)，判别式自监督学习(对比学习)。

> 它的指导原则是：通过自动构造相似实例和不相似实例，要求习得一个表示学习模型，通过这个模型，使得相似的实例在投影空间中比较接近，而不相似的实例在投影空间中距离比较远。

#### 解决的问题是什么？

标注的数据有限，想要更好的效果就需要有更多的数据；解决的问题就是使用自监督的形式来学习知识并应用到下游任务中。

#### 基于负例的对比学习方法

SimCLR看着有很多构件，比如Encoder、Projector、图像增强、InfoNCE损失函数，其实我们最后要的，只是Encoder，而其它所有构件以及损失函数，只是用于训练出高质量Encoder的辅助结构。那么为什么需要两次非线性映射（Encoder、Projector）？图取自[公众号，源头在哪我就不考究了](https://mp.weixin.qq.com/s/zNOymBLuMRO3pnMCMyiykw)。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210428095209-微信图片_20210428094928.jpg)

Moco V2 在整个无标注训练数据集合内，随机选择任意大小的数据，来做为模型训练时的负例。吸收了SimCLR的Projector结构，以及更具难度的图像增强方法之后，针对Moco 的改进版本，提出了 Moco V2。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210428135146-微信图片_20210428135134.jpg)

最主要的特点和创新在两个分枝中的下分枝：SimCLR里上下两个分枝是对称的，两者可参数共享，而Moco V2的下分枝模型参数更新，则采用了动量更新（Momentum Update）机制。

目前主流的基于负例的对比学习方法，主要是这两类：以SimCLR为代表的Batch内负例，以及以Moco为代表的全局选择负例方法。

SwAV的模型结构，其中的图像增强、Encoder以及Projector结构，与SimCLR基本保持一致。跟前面的算法不同的在于怎样比较两个实例之间的区别，SwAV 使用的是使用聚类的办法，让增强后的 Aug1 和 Aug2 聚类到一类里面，而希望这两个正例不被分到其他的类里面（不过我还是不理解隐形的负例是什么意思？而它又是怎样方式模型坍缩的？）

DeepCluster是更早出现的采用两阶段聚类的自监督模型，SwAV论文中对DeepCluser进行了改造，形成了DeepCluser-V2模型。从概念上，可以简单将DeepCluser-V2理解为和SwAV整体结构类似的工作，只不过SwAV对每个Batch数据在线聚类，而DeepCluster-V2是每个Epoch做一次更大规模的聚类。目前来看，SwAV和DeepCluster-V2是效果最好的对比学习模型之一。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210428155224-微信图片_20210428155213.jpg)

#### 非对称模型

BYOL 它做到了！

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210430101511-BYOL.jpg)

对于BYOL来说，它的优化目标要求Online部分的正例，在表示空间中向Target侧对应的正例靠近，也即拉近两组图像增强正例之间的距离，对应Loss 函数为：

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210430101625-image.png)

同样是 Cosine 相似性的一个变体，之后将两个正例交换位置，重新计算 loss，也就得到了：

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210430101821-image.png)

**问题是**：既然BYOL只用正例，它是如何防止模型坍塌的呢？背后的原因，目前仍然是「未解之谜」，不过对此也有些研究进展。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210430102155-simsiam.jpg)

SimSiam 去除了动量更新机制，但是效果并没有之前好。

#### 冗余消除损失

Barlow Twins 没有使用非对称结构，也没有使用动量机制，它既没有使用负例，主要靠替换了一个新的损失函数，可称之为「冗余消除损失函数」，来防止模型坍塌。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210430102535-barlow-twins.jpg)

Barlow Twins 在 Batch 维度，对 Aug1 和 Aug2 里的正例分别做了类似 BN 的正则。之后，顺着 Batch 维，对 Aug1 和 Aug2 两个正例表示矩阵做矩阵乘法，求出两者的互相关性矩阵（cross-correlation matrix），其损失函数定义在这个互相关矩阵 C 上。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210430111736-image.png)

通过尽可能消除表示向量里各个bit位之间的冗余信息表达，起到类似负例的作用，避免模型坍缩。

**不理解的部分**

1. 双塔结构是什么？由上下两个分支构成，也叫做 Branch。
2. Moco v2 是怎么维护的，负例队列的形式是什么样的？
3. SwAV 使用 Sinkhorn-Knopp 算法对 Batch 内数据进行聚类？这是个什么样的算法？
4. few-shot, many-shot 是啥？

#### 面临的挑战是什么？

如何构造相似实例，以及不相似实例，如何构造能够遵循上述指导原则的表示学习模型结构，以及如何防止模型坍塌(Model Collapse)，这几个点是其中的关键（这些会在接下来的发展中尝试解决）。

**训练数据偏置（Bias）问题**；ImageNet 作为一个分类数据集跟现实生活中的多场景照片有一定的差别，参考 CASTing Your Model:Learning to Localize Improves Self-Supervised Representations 这篇文章。

**更好构建正例的方法**；目前的图像增强方法有限，在视角不变性、照明不变性等，对比学习模型的效果要明显弱于监督学习。

**像素级学习能力**；对于需要像素级处理的子领域任务，需要有改进。

原文也提出了更多的问题：

比如只使用正例的对比学习模型，从原理上讲，到底为何模型能够不坍塌？再比如，目前很多探索，集中研究对比学习中的Hard负例问题。我们从上文讲解可知：对于负例对比学习方法，之所以负例越多模型效果越好，其实本质上，是因为越多负例，会包含更多的Hard负例，而这些Hard负例对于模型贡献较大，而easy负例，其实没多少贡献。但是，我们又知道，温度超参本身其实是可以聚焦在Hard负例上的。那么，Hard负例应该研究什么具体问题？这个需要仔细考虑。再比如，目前不少研究在考虑融合有监督模型和对比学习，试图兼具两者的优点，这个有多大意义？

#### 学到了

1. **损失函数：InfoNCE Loss**
   文中提到：如果对比学习的参数设置的不好，很容易出现模型坍缩的情况。首先，距离函数 S 可以计算正例之间、正负例之间的相似性，所以利用这一点，就可以约束 Alignment、Uniformity。把正例的距离作为分子，提升 Alignment，负例的距离放在分母，提升 Uniformity。其中文中也着重介绍了温度参数 T 对于两个性质平衡之间的影响。
   ![图片1](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210427181249-image.png)
2. **距离表示函数**
   采取对表示向量L2正则后的点积或者表示向量间的Cosine相似性，相当于把特征映射到单位超球面上，详情见文章内分析；
   ![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210427181218-image.png)
3. **对比学习的效果衡量标准**
   好的对比学习系统应该具备两个属性：Alignment（分布均匀）和 Uniformity（保留更多信息），其实道理很简单：分布均匀意味着两两有差异，也意味着各自保有独有信息，这代表信息保留充分；也就是说，最好的情况是每个正例都尽可能接近一个点，同时不同正例的点要分布的尽量均匀。
4. **移动平均（Moving Average）机制和动量更新（Momentum Update）机制**
   Moco v2 下分支模型参数的更新，并不是通过常规的损失函数反向传播来进行梯度更新，而是采用如下的移动平均（Moving Average）机制进行更新.
   ![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210428135907-image.png)
   m 是权重调节参数，一般取得比较大（0.9甚至0.99），所以更新会非常的缓慢而稳定，小步慢走。

[吴恩达深度学习专项新增 Transformer](https://www.deeplearning.ai/program/deep-learning-specialization/)
