---
title: ACL2021实体关系提取论文阅读笔记
date: 2021-08-22 14:42:07
permalink: /2021-08-22-week-post/
cover: https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210820175248-imagepng
tags: 
- 周报
- 笔记
categories: 周报
abstract: 本周主要将ACL2021中关于关系提取的几篇论文拿出来读一读，并且简单记录一下这些论文的创新点与缺点。
---
本文主要介绍以下四篇论文的进展情况，这里是参考了娄杰大佬整理的ACL清单：[分类汇总 ｜ ACL2021 信息抽取相关论文 (qq.com)](https://mp.weixin.qq.com/s/tLX117eblU8U60qIwhudMg)；更多关系提取相关的论文请参考[这里](https://www.xerrors.fun/2021-08-11-week-post/)的文章，其中第三节部分主要介绍了几种主流的实体关系提取的方法。

顺便说两句，这四篇文章的第一作者都是中国人（看名字和单位应该是），难道国外的大佬都不做关系提取的吗？

## 1. 总览

主要包括：

- [PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction (aclanthology.org)](https://aclanthology.org/2021.acl-long.486.pdf)
- Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference
- UniRE: A Unified Label Space for Entity Relation Extraction
- Dependency-driven Relation Extraction with Attentive Graph Convolutional Networks

## 2. 详细笔记

此段可略过：闲扯两句，看了这么久才发现，这些论文的参考文献是按照姓氏的首字母排序的！不过确实这样比较合理，我特别喜欢这种参考文献的方式，文中能够直接看到作者的姓氏和年份，这样就能够判断是不是自己之前看过的论文，如果不是很确定，也可以快速的按照姓氏到后面的参考文献里面去比对论文的标题！

### 基于潜在关系和全局对应关系（PRGC）

论文在此：[PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction (aclanthology.org)](https://aclanthology.org/2021.acl-long.486.pdf)

先看摘要：

> 从非结构化文本中联合提取实体和关系是信息提取的一项重要任务。最近的方法取得了可观的性能，但仍有一些固有的局限性，如**关系预测的冗余性**，基于跨度的提取的**通用性差和效率低下**。
>
> 在本文中，我们从一个**新的角度**将这一任务分解为三个子任务，即**关系判断**、**实体提取**和**主宾语对齐**，然后提出了一个**基于潜在关系和全局对应关系（PRGC）的联合关系三元组提取框架**。
>
> 具体来说，我们设计了一个预测潜在关系的组件，它将下面的实体提取限制在预测的关系子集上，而不是所有的关系；然后应用一个特定关系的序列标签组件来处理主体和客体之间的重叠问题；最后，设计了一个全局对应组件来将主语和宾语以**低复杂度**对齐成一个三元组。
>
> 广泛的实验表明，PRGC在公共基准上实现了最先进的性能，具有更高的效率，并在三元组重叠的复杂场景中实现了一致的性能提升。

在我看来，文章的出发点在于解决之前模型的计算效率低下问题，首先作者认为基于跨度的提取方法仅仅关注**实体的起止坐标**，导致泛化性能很差（作者未做解释）；作者还认为当前 SOTA 的 TPLinker 在每个句子的每个关系都使用了两个 $O(n^2)$ 的矩阵，导致极度的冗余，同时还拥有基于跨度算法的通病。

下面是模型的结构图，重点是画虚线的是三个部分；摘要中，实体关系提取任务分成了三个部分，分别对应图中的三个组件：

1. 关系判断：（橙色虚线）Potential Relation Prediction
2. 实体提取：（蓝色虚线）Relation-Specific Sequence Taggers
3. 主宾语对齐：（绿色虚线）Global Correspondence

请允许我吐槽一下论文中各个公式的写法，字母混用！公式(1)(2)(3)全部都是使用字母 P 来表示输出，我不理解为什么要这样；比如图中橙色部分的输出，即句子中可能存在的关系，是使用 $\mathbf{R}^{pot}$ 来表示的，而公式中是使用 $P_{rel}$ 表示这个数组中一个元素的；公式乱的一塌糊涂！接下来的所有公示中，我会使用 $\mathbf{P}_{sub, obj} \in \mathbb{R}^{n \times n}$ 来表示实体对应关系；使用 $\mathbf{R}^{pot} \in \mathbb{R} ^ {n_r \times 1}$ 来表示潜在的关系向量；使用 $\mathbf{E}^{sub},\mathbf{E}^{obj} \in \mathbb{R}^{n \times m}$；其中 $n$ 是句子序列中的 tokens 的个数，$n_r$ 表示所有的关系的个数，$m$ 的是指潜在的关系的个数；

在介绍具体的公式之前，我先简述一下整个过程；首先是对输入的句子进行编码，得到 $\mathbf{h}$ ，这时候同时执行两个操作，即图中的绿色部分和橙色部分，绿色部分表示全局对应关系，是个 $n \times n$ 的矩阵，表示两个位置之间的主宾语对应关系，有关系为 1 没关系为 0；橙色部分表示预测潜在的关系，是个长度为 $n_r$ 的向量，表示所有的关系的状态，标记 1 表示是这个句子中可能存在的关系，0 表示不是这个句子中可能存在的关系（这个想法真的不错）。

然后针对已经提取出来的潜在关系以及句子编码 $h$ ，执行图中蓝色虚线部分的任务，针对可能存在的 $m$ 个关系中的每个关系，都计算出该关系对应的主语和宾语的实体标记（BIO），可能有多个，得到一个序列表 $ 2\times n$；

所以现在我们数一数我们有什么，我们有实体关系矩阵表、潜在的关系表、全局对应表；然后对于这三个输出对照标签数据计算他们的交叉熵损失，加权求和之后就是最终的损失。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210822212611-imagepng)

**关系判断**：输入是句子的特征向量 $\mathbf{h}$，这个是 BERT 等预训练模型编码器的输出，然后输出是 $\boldsymbol{R}^{pot}$；是一个一维的向量，

$$
\begin{aligned}
\mathbf{h}^{\text {avg }} &=\text { Avgpool }(\mathbf{h}) \in \mathbb{R}^{d \times 1} \\
\mathbf{R}^{pot}_{i} &=\sigma\left(\mathbf{W}_{r, i} \mathbf{h}^{\text {avg }}+\mathbf{b}_{r,i}\right)
\end{aligned}

$$

其中 $\mathbf{W}_{r} \in \mathbb{R} ^ {d \times n_r}$ 和 $\mathbf{b}_r$ 是可训练的参数；就是说这个组件进行的计算就是一个平均池化、一个全连接层和一个激活函数，从 sigmoid 函数可以看出来，这是一个多标签的二分类问题；

**实体提取**：输入是句子的特征向量 $\mathbf{h}$，以及潜在的关系 $\mathbf{R}^{pot}$；

$$
\begin{array}{r}
\mathbf{E}_{i, j}^{sub}=\operatorname{Softmax}\left(\mathbf{W}_{s u b}\left(\mathbf{h}_{i}+\mathbf{u}_{j}\right)+\mathbf{b}_{s u b}\right) \\
\mathbf{E}_{i, j}^{obj}=\operatorname{Softmax}\left(\mathbf{W}_{o b j}\left(\mathbf{h}_{i}+\mathbf{u}_{j}\right)+\mathbf{b}_{o b j}\right)
\end{array}

$$

其中 $\mathbf{u}_j \in \mathbb{R}^{d \times 1}$ 表示可训练学习的矩阵 $\mathbf{U} \in \mathbb{R}^{d \times n_r}$ 第 $j$ 个关系表征；总的来说，也就是一个全连接一个激活函数就完成了。公式中的 $j$ 是使用 $\boldsymbol{R}^{pot}$ 为 1 的位置的 $j$；

**主宾语对齐**：这里是计算主语和宾语的对应关系，是一个二维矩阵，矩阵中的每个元素 $\mathbf{P}_{i, j}$ 表示第 $i$ 个起始坐标的实体和第 $j$ 个起始坐标的实体是主语和宾语关系的置信度；具体计算方法就是把两个实体的 token 直接拼接之后经过一个全连接和 sigmoid 激活函数。这里的对齐过程是可以跟关系预测过程（橙色部分）并行进行的。

$$
\mathbf{P}_{i, j} = \sigma (\mathbf{W}_{g} [ \mathbf{h}_{i}; \mathbf{h}_{j}] + \mathbf{b}_{g})

$$

终于介绍完这些恶心的公式了，明明很简单的任务却要写的这么复杂，直接说自己额外创建了几个子任务并针对子任务定制了损失函数来达到多任务学习的目的；全文连一个多任务都没出现，不知道是怎么想的。

实验结果：

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210823101006-imagepng)

接下来说说这个实验结果的对比，嗯很好，不过我不太喜欢作者在文中所提到的：

> 有一点很重要的是，尽管 TPlinker 比 CasRel 的参数量要多，但是在 WebNLG 数据集上却仅仅提升了 0.1%，并且 TPLinker 作者把这个问题归咎于数据集本身的问题；然而，我们的模型在 WebNLG 数据集上得到了 10 倍于 TPLinker 的提升效果；

我承认，你使用潜在的关系这个做法非常的巧妙，也大大减少了参数量和计算量，但是你这 1.2% 的**性能提升**，不值得你这么吹牛吧 ，更何况你这 93% 的分数也不是 SOTA 吧，你为什么不对比 2020 年就已经登顶的 SOTA [SPN]([arxiv.org](https://arxiv.org/abs/2011.01675v2)) 呢？而且榜单([Relation Extraction | Papers With Code](https://paperswithcode.com/task/relation-extraction))上也没你啊。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210823101915-imagepng)

好的，那我们来比对一下**计算效率问题**：

图 a 取自 TPLinker Table 4：[2010.13415.pdf (arxiv.org)](https://arxiv.org/pdf/2010.13415.pdf)

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210823104832-imagepng)

图 b 取自 PRGC Table 6：[PRGC (aclanthology.org)](https://aclanthology.org/2021.acl-long.486.pdf)

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210823104920-imagepng)

图 a 是  TPLinker 所放出的参数量以及推理时间的结果，图 b 是 PRGC 论文中的结果。这里的参数量你告诉我有什么意义呢？你不把 BERT 的参数量算进去，你只看后部分的参数量和推理时间并没有太大的实战意义。

那么最后就是**通用性**泛化能力的问题了，不得不说作者的实验做的是真滴牛批，

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210823115942-imagepng)

这里是做实验说明自己的 Rel-Spec Sequence Tagging 模块比 span-base 的模块效果要好，注意看第二行，找你这么说，这个模块还真是差到离谱了；你把 AK-47 的弹夹换成 M2 的弹夹，发现效果变化很大，然后你就得出来 M2 不如 AK-47 是吗？就离谱（我不懂军事，我就是举个例子，不能通过单纯换组件来证明自己的观点，毕竟你们连个模型在策略上就差别很大）；你可以说你的这个新模块是有用的，是可行的；而不是吹自己踩别人，毕竟你这样的实验并不具备说服力。

再看看你这举的例子，你觉得有说服性吗？什么时候比较通用性可以通过一两个示例来说明了？你就全是对的，别人的就全是错的？更何况你这两个例子跟通用性有什么关系？我这边建议作者有空看看这篇论文：[1283_paper.pdf (ecai2020.eu)](https://ecai2020.eu/papers/1283_paper.pdf)，同样是写论文，你看看这气度，人家没有踩别人，还仔细分析了自己的错误示例，还给之后的研究指出了方向，以至于后续刚好有人在他的基础上做出了更好的效果。而不是像你这样心胸狭隘！

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210823120439-imagepng)

> 通过图3（上图）所示的案例研究，我们发现 span-based 方案倾向于提取长实体并识别正确的主宾语对，但忽略了它们之间的关系。这是因为模型倾向于记住实体的位置，而不是理解底层语义（猜测而已）。然而，PRGC使用的序列标记方案**在这两种情况下**都表现良好，实验结果证明我们的标记方案更具健壮性和通用性。

**作者结论**：

> 本文提出了一种全新的视角，提出了一种基于潜在关系和全局对应的联合关系抽取框架，极大地缓解了冗余关系判断、基于广度的抽取泛化能力差和主宾语对齐效率低的问题。实验结果表明，我们的模型在公共数据集中实现了最先进的性能，并以更高的效率成功地处理了许多复杂场景。

**我的结论**：

嗯，就很无语！一个劲的吹自己的效率高，速度快。我都要被恶心到了！啥也不是！

### 利用知识增强的集体推理进行生物医学实体和关系的联合提取

论文在此：[Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference (aclanthology.org)](https://aclanthology.org/2021.acl-long.488.pdf)

先看摘要：

> 与一般的新闻领域相比，**生物医学文本的信息提取**（IE）需要更广泛的领域知识。然而，以前的许多 IE 方法在推理过程中没有利用任何外部知识。由于生物医学出版物的指数级增长，那些没有超越其固定参数集的模型很可能会落后。受人类如何查找相关信息以理解科学文本的启发，我们提出了一个**新的框架，利用外部知识进行联合实体和关系提取**，命名为**KECI（知识增强集体推理）**。给定一个输入文本，KECI首先构建一个**初始跨度图**，代表其对文本的初始理解。然后，它使用一个**实体链接器**来形成一个**知识图**，包含文本中提到的实体的相关背景知识。为了做出最终的预测，**KECI使用注意力机制将初始跨度图和知识图融合成一个更精细的图**。KECI采取了一种集体的方法，通过**使用图卷积网络**将全局关系信息整合到局部表示中，将提及跨度与实体联系起来。我们的实验结果表明，该框架是非常有效的，在两个不同的基准数据集中取得了新的先进成果。BioRelEx（结合作用检测）和ADE（药物不良事件提取）。例如，在BioRelEx实体和关系提取任务中，KECI的F1得分比最先进的技术分别提高了4.59%和4.91%。

由于图卷积和生物医学文本的信息提取并不是很了解，所以这一篇文章主要是一个略读的简单笔记。

> 总的来说，KECI的设计部分地受到了以前教育心理学研究的启发。学生的背景知识在指导他们对科学文本的理解和理解方面起着至关重要的作用（Alvermann等人，1985；Braasch和Goldman，2010）。"激活 "相关和准确的先前知识将有助于学生的阅读理解。

有一说一，这个作者的公式写的是真的清楚，解释的也清楚，很好理解！

把文本变成关系三元组，总共分四步：

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210823214226-imagepng)

**第 0 步，跨度编码**；

即把句子中所有长度不超过 L 的跨度进行编码，编码的方式很简单，使用该跨度一些基本信息传入到一个前馈神经网络里面就行了，这些基本信息包括：起始位置的特征向量、结束位置的特征向量、该跨度表征中的注意力加权和以及表示跨度长度的特征向量；每个跨度的输出的维度是 d。

$$
\mathbf{s}_{i}=\mathrm{FF} \mathbf{N N}_{\mathrm{g}}\left(\left[\mathbf{x}_{\mathrm{START}(i)}, \mathbf{x}_{\mathrm{END}(i)}, \hat{\mathbf{x}}_{i}, \phi\left(\mathbf{s}_{i}\right)\right]\right)

$$

**第 1 步，构建初始的跨度图**；

预测实体以及预测关系，有了每个跨度的特征向量之后，直接预测这些跨度是哪些实体，跨度之间两两是否有关系；

$$
\begin{aligned}
\mathbf{e}_{i}&=\operatorname{Softmax}\left(\mathrm{FFNN}_{e}\left(\mathbf{s}_{i}\right)\right) \\
\mathbf{r}_{i,j}&=\operatorname{Softmax}\left(\operatorname{FFNN}_{r}\left(\left[\mathbf{s}_{i}, \mathbf{s}_{j}, \mathbf{s}_{i} \circ \mathbf{s}_{j}\right]\right)\right)
\end{aligned}

$$

对于每一个跨度而言，实体的输出 $e_i$ 是该跨度 $s_i$ 是某个实体的概率，一个长度为 $|E|$ 的概率向量；而关系的输出是一个全局的矩阵，假设有 $m$ 个跨度，那么矩阵的维度就是 $m\times m \times |R|$，$r_{i,j}[k]$ 表示跨度 $s_i$ 和 $s_j$ 具有关系 $k$ 的概率；

有了这些之后我们不就得到最后的输出了吗（不是）；接下来才是重点，接下来把每个跨度作为一个节点，把这些节点全部两两用 $|R|$ 条线连起来，每两个跨度之间的每个关系的概率就是每根线的权重，这不就构成了一个大图了吗；

之后就利用这些节点和边使用双向 GCN 来迭代更新每个节点的值，更新迭代后的节点的值使用 $\mathbf{h}_i$ 表示；

$$
\begin{aligned}
\overrightarrow{\mathbf{h}_{i}^{l}} &=\sum_{s_{j} \in V_{s} \backslash\left\{s_{i}\right\}} \sum_{k \in R} \mathbf{r}_{i j}[k]\left(\overrightarrow{\mathbf{\mathbf { W }}_{k}^{(l)}} \mathbf{h}_{j}^{l}+\overrightarrow{\mathbf{b}_{k}^{({l})}}\right) \\

\overleftarrow{\mathbf{h}_{i}^{l}} &=\sum_{s_{j} \in V_{s} \backslash\left\{s_{i}\right\}} \sum_{k \in R} \mathbf{r}_{j i}[k]\left(\overleftarrow{\mathbf{W}_{k}^{(l)}} \mathbf{h}_{j}^{l}+\overleftarrow{\mathbf{b}_{k}^{(l)}}\right)\\

\mathbf{h}_{i}^{l+1} &=\mathbf{h}_{i}^{l}+\mathrm{FFNN}_{a}^{(l)}\left(\operatorname{ReLU}\left(\left[\overrightarrow{\mathbf{h}_{i}^{l}}, \overleftarrow{\mathbf{h}_{i}^{l}}\right]\right)\right)
\end{aligned}

$$

**第 2 步，构建背景知识图**；

嗯，看不懂，下面的可以不看，只需要知道得到了一个图 $G_k$；节点是 $n_i$；

> 我们首先使用MetaMap从输入文档D中提取 UMLS 生物医学实体，这是一个UMLS的实体映射工具（Aronson and Lang, 2010）。然后我们从提取的信息中构建一个背景知识图（KG）。
>
> 更具体地说，我们首先为每个提取的**生物医学实体**创建一个**节点**。每个实体节点的**语义类型**也被建模为**类型节点**，**与相关的实体节点相连**。
>
> 最后，我们为元词库和语义网络中发现的每一个相关关系创建一条边。图2中的灰色阴影区域就是一个KG的例子。圆圈代表实体节点，矩形代表对应于语义类型的节点。
>
> 请注意，我们只是用默认选项运行MetaMap，并没有对其进行调整。在我们的实验中，我们发现，MetaMap通常会返回许多与输入文本无关的候选实体。然而，正如第3.4节所讨论的，实验表明 KECI 可以学会忽略不相关的实体。
>
> 让$G_k= \{V_k, E_k\}$表示构建的背景 KG，其中$V_k$和$E_k$分别是节点和边集。我们使用由Maldonado等人（2019）预训练的一组 UMLS 嵌入来初始化 $V_k$ 中每个节点的表示。我们还使用SciBERT 将每个节点的 UMLS 定义句子编码为一个向量，并将其与初始表示相连接。之后，由于$G_k$是一个异质关系图，我们使用关系型GCN（Schlichtkrull等人，2018）来更新每个节点 $v_i$ 的表示。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210823214552-imagepng)

> 在进行多次信息传递的迭代后，KG的全局关系信息将被整合到每个节点的表示中。$v_i$ 表示关系型 GCN 的最后一层的特征向量。我们使用一个简单的前馈网络将每个向量 $v_i$ 进一步投射到另一个向量 $n_i$ 上，这样 $n_i$ 的维度与跨度表示相同。

**第 3 步，图预测;**

不管看没看懂，现在有两个图了 $G_s$ 和 $G_k$；那么接下来，我们首先计算出 $G_s$ 的每个跨度节点的候选实体 $C(s_i)$（怎么算的？），即结构图中每个节点虚线相连的部分，同时针对每个跨度节点与候选实体计算他们之间的相关分数；同时计算一个额外的 **sentinel vector**（守卫向量？）以及相关分数；这里的守卫向量记录了跨度的本地语境信息，守卫向量的相关分数表示这个信息的重要程度。之后就能得到每个跨度 $s_i$ 的**知识感知**的特征向量 $\mathbf{f}_i$；

这个注意力机制计算方法如下图和公式所示，看图理解就好理解了：

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210823221318-imagepng)

这时候上面的 $\mathbf{f_i}$ 就包含了之前的全局的实体关系表征信息、与背景知识图谱对应的实体的知识信息、候选实体的加权信息，所以是时候直接利用这些表征来预测真正的实体和实体之间的关系了！

$$
\begin{aligned}
\widehat{\mathbf{e}_{i}} &=\operatorname{Softmax}\left(\mathrm{FFNN}_{\widehat{e}}\left(\mathbf{f}_{i}\right)\right) \\
\widehat{\mathbf{r}_{i j}} &=\operatorname{Softmax}\left(\operatorname{FFNN}_{\widehat{r}}\left(\left[\mathbf{f}_{i}, \mathbf{f}_{j}, \mathbf{f}_{i} \circ \mathbf{f}_{j}\right]\right)\right)
\end{aligned}

$$

计算方法跟前面相似；

那么关于损失函数就是同时包含了初始的那次预测的交叉熵损失以及最终的这次预测的交叉熵损失，只是后者的权重比较高。

$$
\mathcal{L}_{\text {total }}=\left(\mathcal{L}_{1}^{e}+\mathcal{L}_{1}^{r}\right)+2\left(\mathcal{L}_{2}^{e}+\mathcal{L}_{2}^{r}\right)

$$

**实验结果**：

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210823222750-imagepng)

除了之前的几个模型以外，作者还创建了几个额外的基线模型，比如只使用本地的语境来预测 SentContextOnly、不使用图卷积 FlatAttention、把第二部分即灰色的那一块替换成 KnowBertAttention 模块，这个模块具体的我就不琢磨了，可以参考[这里](https://doi.org/10.18653/v1/D19-1005)；

上面的数据已经可以说明效果了，那么作者也指出了目前模型所存在的问题，即数据集中的实体提及和UMLS实体之间没有 gold 标准的对应关系集。因此，不能直接评估 KECI 的实体链接性能。但是作者记录了该类型的实体被分配的平均注意力权重（没读懂这是啥），发现模型最关注相关的信息实体，而忽略不相关的实体。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210823224103-imagepng)

下图的三个例子说明基于 UMLS 的图谱可以正确的连接实体的关系、减去不存在的关系、修正错误的实体类型；但是一个**主要问题**是，MetaMap有时不能从 UMLS 中为一个实体的 mention 返回任何候选实体。我们把这项工作扩展到使用多个知识库作为未来的工作。

**结论**：

这篇文章是我见到的第一个把相关工作放在实验后面的；

> 在这项工作中，我们提出了一个新的基于跨度的框架，名为KECI，利用外部领域知识从生物医学文本中联合提取实体和关系。实验结果表明，KECI是非常有效的，在两个数据集上取得了新的先进的结果。BioRelEx和ADE。理论上，KECI可以将整个文档作为输入；但是，测试的数据集只是句子级的数据集。在未来，我们计划在更多的文档级数据集上评估我们的框架。我们还计划探索更广泛的可以从外部知识库中提取的属性和信息，以促进生物医学的 IE 任务。最后，我们还计划将KECI应用于其他信息提取任务；

我的结论：

作者的想法以及实验结果都是有效的，是个很有意义的尝试（当然可能并不是独创的）；不过也确实可能说明利用外部的背景知识可以针对特定领域的任务进行优化；而且这种优化也不需要很多的人工成本，是可以简单的把相同的方法用在其他领域的。
