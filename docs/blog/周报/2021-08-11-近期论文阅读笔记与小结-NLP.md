---
title: 近期论文阅读笔记与小结-NLP
date: 2021-08-11 21:57:05
permalink: /2021-08-11-week-post/
cover: https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210820175248-imagepng
tags: 
- 周报
categories: 周报
abstract: 这周主要是看的关系提取的论文，主要包括多模态预训练方法的ViLBERT、UNIMO；利用对比学习的实体和关系理解；
---
其实这里写的很乱，里面包括了很多篇论文，主要还是第三节里面有很多关于实体关系提取方法的介绍，目前有 5 篇论文，本篇文章尽量将论文的核心思想以及优缺点给讲清楚，仅供参考！

## 1. 多模态预训练方法

### ViLBERT

这个了解不多，首先是看到了这个论文：面向视觉基础进行预训练！（第一次看到这么调皮的标题）[[1908.02265] ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks (arxiv.org)](https://arxiv.org/abs/1908.02265)

![ViLBERT](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210811220542.png)

这里是解读文章：[BERT新转变：面向视觉基础进行预训练！ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/101511981)，很不错，原文实在是晦涩难懂！

![](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210811220755.png)

![](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210811220822.png)

![](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210811220842.png)

### UNIMO

解读文章：[机器之心](https://mp.weixin.qq.com/s/CUR57R-4xXHpVLFCJTG55w)，原谅我看不懂原文！

![](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210811221154.png)

> **百度提出面向异构模态数据的统一预训练方法 UNIMO**，在具体训练过程中，文本、图像和图文对**三种模态数据随机混合**在一起，其中图像被转换为目标（object）序列，文本被转换为词（token）序列，图文对被转换为目标序列和词序列的拼接。
>
> UNIMO 对三种类型数据进行统一处理，在目标序列或者词序列上基于掩码预测进行**自监督学习**，并且基于图文对数据进行**跨模态对比学习**，从而实现图像与文本的统一表示学习。进一步的，这种联合学习方法也让文本知识和视觉知识互相增强，从而有效提升文本语义表示和视觉语义表示的能力。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210811222953-_20210811222737jpg)

> 异构模态的统一预训练最大的挑战是如何跨越不同模态间的语义鸿沟从而实现语义表示的统一。如下图所示，UNIMO 提出了创新的**跨模态对比学习**方法，同时引入相关联的图文对数据、文本数据和图像数据进行**联合对比学习**。具体地，UNIMO 通过文本改写的方式，对图文对进行数据增广，获得大量的正例和强负例图文对数据。同时为了更好的利用文本和图像数据，UNIMO 通过文本与图像检索，获得相关的图像和文本作为正例。这样利用扩充后的多种类型的正例以及高质量强负例，**UNIMO 在统一的语义空间上进行联想对比，从而能够学习到精确对齐的跨模态语义表示**。注：玄学！

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210811223408-_20210811222745jpg)

下面一起来看看实验是怎么做的，反正就是很多很多数据。

> 在实验方面，UNIMO 使用了大量的文本、图像和图文数据进行联合学习，同时在各种单一模态和跨模态下游任务上进行验证。预训练数据部分，**文本语料**包括 Wikipedia、BookCorpus、OpenWebText 等共 54G 语料；**图像数据**是从互联网爬取的 170 万张图像；而**图文对数据**则包括 COCO Caption、Visual Genome、Conceptual Caption、SBU Caption。
>
> 下游任务既包括图文搜索、视觉问答、图描述生成、视觉推断等跨模态任务，也包括文本分类、阅读理解、文本摘要、问题生成等各种文本任务。模型上，Base 基于 12 层的 Transformer，而 Large 使用 24 层。

看也看不懂，只能喊666了

## 2. 利用对比学习的实体和关系理解

论文在此：[[2012.15022] ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning (arxiv.org)](https://arxiv.org/abs/2012.15022)

这篇文章所想要解决的痛点就是：传统的预训练目标没有明确地对文本中的关系事实进行建模，而这些对文本理解至关重要。

为了解决这个问题，定义了两个**新的预训练任务**来更好地理解实体和关系。

1. 实体辨别任务（ED），区分哪个尾部实体可以由给定的头部实体和关系推断出来；
2. 关系辨别任务（RD），区分两个关系在语义上是否接近，这涉及复杂的关系推理；

那么对于这两个新的任务，是不是就要有新的数据集啊，那么很抱歉并没有，所以想要使用对比学习的方法来制造负样本来完成训练。具体而言：

### Entity Discrimination Task

如图所示，对于数据集（远程监督得到的）中实体关系数据，目前已知 $(d,r,e_1,e_2)$，然后把 $(d,r,e_1)$ 输入到模型里面，让模型猜，模型就会在这个文档 $d$ 中所有出现的实体中猜，如果猜对了就加分，猜错了就减分。那么$e_1$就是正样本，其余实体都是负样本。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210815151835-imagepng)

写成数学公式就是：

$$
\mathcal{L}_{\mathrm{ED}}=-\sum_{t_{j k} \in \mathcal{T}^{+}} \log \frac{\exp \left(\cos \left(\mathbf{e}_{ j}, \mathbf{e}_{ k}\right) / \tau\right)}{\sum_{l=1, l \neq j}^{\left|\mathcal{E}\right|} \exp \left(\cos \left(\mathbf{e}_{ j}, \mathbf{e}_{ l}\right) / \tau\right)}

$$

具体的每个字母的含义是什么，就需要仔细看论文了。

### Relation Discrimination Task

对于这个任务，也是使用了对比学习。首先需要注意的是，这里的关系的表示是利用实体拼接得来的，实体的编码表示是使用 mean pooling 得到的；这里的计算方法是一样的，对于相同的关系我就加分，不同的关系我就减分，这里的关系来源可以是全部来自句内关系，也可以来自跨句关系。所以就有了这个看不懂的图和看不懂的公式了。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210815152940-imagepng)

$$
\begin{aligned}
\mathcal{L}_{\mathrm{RD}}^{\mathcal{T}_{1}, \mathcal{T}_{2}} &=-\sum_{t_{A} \in \mathcal{T}_{1}, t_{B} \in \mathcal{T}_{2}} \log \frac{\exp \left(\cos \left(\mathbf{r}_{t_{A}}, \mathbf{r}_{t_{B}}\right) / \tau\right)}{\mathcal{Z}} \\
\mathcal{Z} &=\sum_{t_{C} \in \mathcal{T} /\left\{t_{A}\right\}}^{N} \exp \left(\cos \left(\mathbf{r}_{t_{A}}, \mathbf{r}_{t_{C}}\right) / \tau\right) \\
\mathcal{L}_{\mathrm{RD}} &=\mathcal{L}_{\mathrm{RD}}^{\mathcal{T}_{s}^{+}, \mathcal{T}_{s}^{+}}+\mathcal{L}_{\mathrm{RD}}^{\mathcal{T}_{s}^{+}, \mathcal{T}_{c}^{+}}+\mathcal{L}_{\mathrm{RD}}^{\mathcal{T}_{c}^{+}, \mathcal{T}_{s}^{+}}+\mathcal{L}_{\mathrm{RD}}^{\mathcal{T}_{c}^{+}, \mathcal{T}_{c}^{+}}
\end{aligned}

$$

这里多提一句，这个 $\mathcal{Z}$ 所表示的其实就是哪些不同关系的得分。

### 总结

最后呢，还使用了一个不知道是什么玩意的损失函数，这个下面这个样子，叫做：

> 为了避免对一般语言的灾难性遗忘理解能力，我们在训练掩盖的语言建模任务($\mathcal{L}_{\mathrm{MLM}}$)，同时训练ED和RD 任务。（听着就像是一般的 mask 训练）

$$
\mathcal{L}=\mathcal{L}_{\mathrm{ED}}+\mathcal{L}_{\mathrm{RD}}+\mathcal{L}_{\mathrm{MLM}}

$$

从整个论文读下来，就感觉他的侧重点其实是长文本或者文档级的语义理解，然后恰好使用了一个 mean pooling 的方法使得整体在小样本上的表现更好。跟其他预训练模型一样，关系往往是预先制定好的关系，再然后就去进行实体和关系的分类，但是实际上的任务可能更加复杂，所以在下游任务的微调应用中，并没有很好的改善这一情况。

## 3. 实体关系提取

这是一个全新的任务，之后也会进行详细的梳理，这里只是现将最近的论文的阅读给整理出来。

### 基于Pipeline的PURE

论文在此：[[2010.12812] A Frustratingly Easy Approach for Entity and Relation Extraction (arxiv.org)](https://arxiv.org/abs/2010.12812)

解读在此：[陈丹琦“简单到令人沮丧”的屠榜之作：关系抽取新SOTA！](https://mp.weixin.qq.com/s/xwljKL3FjY-Nw-Zll4x3pQ)

**动机**：目前关于实体关系提取，多数都是采用的联合模型的方法；但是作者任务，对于实体识别和关系提取这两个任务而言，他们的上下文特征肯定是不一样的，如果通过简单的共享参数，一定会对模型的训练产生弊端的。但是另一方面，实体的文本、边界和类型信息也已经被证实对关系提取任务有帮助。

> 长期以来，人们一直认为联合模型可以更好地捕捉实体和关系之间的相互作用，并有助于缓解错误传播问题。

所以就提出来了一个新的策略（似乎并不能够叫做模型）

目前 Pipeline 所存在的问题是：

1. pipeline方式存在误差积累吗，还会增加计算复杂度（实体冗余计算）
2. pipeline方式存在交互缺失，忽略实体和关系两个任务之间的内在联系

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210816161215-imagepng)

对于实体模型，似乎并没有什么改进策略，输入是文本，从所有 span（段）中找到可能的实体；主要是关系提取模块，这里的输入融合了前面得到的实体信息，包括文本标签和类型标签，嵌入在每个实体的前后；但是这样会导致计算开销太大，所以后面就使用了一个新的加速的方法，后面单独记录。

> 之前也有学者重新使用跨度表征 $h_i$ 和 $h_j$ 来预测 $s_i$ 和 $s_j$ 之间的关系，作者假设这些表征只捕捉到每个单独实体周围的上下文信息，可能无法捕捉到一对 spans 之间的依赖关系。我们还认为，在不同的 spans 对之间共享上下文表述可能是次优的。

所以，作者的关系模型独立地处理每一对 spans，并在输入层插入类型标记，以突出主语和宾语及其类型。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210816163957-imagepng)

**在跨句上下文处理**上，使用了一个窗口大小为 W 的机制，具体来说，给定一个有 n 个单词的输入句子，分别从左侧语境和右侧语境中增加 (W-n)/2 个单词。（我不理解为什么 W 的增加会导致关系提取效果变差，原论文只字未提，难道是因为过大的无关文本对关系判断产生了干扰？）

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210816163606-imagepng)

**加速方面**；作者提出，这个方法的主要缺点就是没对实体都要运行一次关系模型，参考图中（b）的这个句子需要执行两次，因为每次的输入并不一样。具体而言就是将实体的文本和实体的类型标签共享相同的位置编码，之后将编码后的标签标记拼接到句子的最后面；同时在注意力层加上约束（这里的约束是指类似于 mask 的东西吗？），使得文本标记只能关注到文本标记，类型标记可以关注到同批次的四个标签和前面的文本标记（个人理解）。

**结论就是**：作者笃定，对于实体关系提取任务里面，简单的共享编码器会导致效果变得更差（起码对于这个模型而言），此外，实体信息可以明显的提升关系提取的效果，但是关系提取的结果并没有对实体提取有明显的帮助。

那么问题来了：

1. 联合模型为什么能够取得更好的效果呢？
2. Pipeline 的反向传播一样也是从后往前进行传播的，具体是怎么训练的？
3. 为什么 W 的增加会导致关系提取效果变差？

### 解决暴露偏差的联合提取 TPLinker

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210817160640-imagepng)

题目：TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking

论文：[2010.13415.pdf (arxiv.org)](https://arxiv.org/pdf/2010.13415.pdf)

参考：[实体关系抽取新范式！TPLinker：单阶段联合抽取，并解决暴漏偏差～ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/346897151)

**划重点**：TPLinker是实体关系抽取的新范式（发表时），巧妙设计了**统一的联合抽取标注框架**，可实现单阶段联合抽取、并解决暴漏偏差，同时依旧可以解决复杂的重叠关系抽取。还解决了训练过程中过于依赖 gold 的问题。

接下来就主要介绍一下这个论文的核心部分，其实也就是两个图和一个解码过程；

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210817161145-imagepng)

试想一下，实体的坐标对应关系有哪些，实体头（$E_{head}$）对应实体尾（$E_{tail}$），主语头（$S_{head}$）与宾语头（$O_{head}$），主语尾（$S_{head}$）与宾语尾（$O_{head}$）；（注：其实好像两个关系就够了哈，实体头与尾，主语头与宾语头，另外一个关系是可以由前两者得到的，就是解码起来会麻烦一点，先不管这么多了）

对于一个句子而言，实体头和实体尾的标记矩阵只需要一个就够了，但是主语与宾语之间的标记矩阵就需要根据关系来确定，每个关系都需要有一个头关系对应表（SH-to-OH）与尾关系对应表（ST-to-OT）；所以对于每一个句子就需要 2*R+1 个矩阵，后来作者想了想太浪费空间了，就改成了上三角矩阵的，把原本下三角部分的值直接折叠上去（如上图右部分），用 2 来表示反的对应关系，毕竟两个实体不会同时出现互为主语与宾语的关系吧（起码在测试集上不会出现，不会出现主动关系与被动关系就是两种关系的情况）。

这里我要吐槽一下这个图画的，图中不同颜色的方块是在不同的矩阵的，针对不同的关系对应的矩阵值也是不一样的，就比如右图中蓝色的 1 跟蓝色的 2 并不在一个矩阵，所以并不是因为 1+1 所以写作 2；对了，作者后来为了计算方便就把上三角矩阵拉平，改成了一个向量（如下图）。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210817162425-imagepng)

也就是模型的输出其实就是这样一个矩阵图，估计算法看了也是一脸懵逼，所以作者还特意写了一个很长的解码算法；（我是万万没想到关系的表示可以复杂到这个成都，这都能直接拿给隔壁信息安全专业做编码解码算法了吧）

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210817163058-imagepng)

看不懂，请拿走谢谢！🙄

这时候就要祭出我的 jay 大佬给的例子了：

> [JayJay - 知乎 (zhihu.com)](https://www.zhihu.com/people/lou-jie-9)
>
> TPLinker的解码过程为：
>
> 1. 解码EH-to-ET可以得到句子中所有的实体，用实体头token idx作为key，实体作为value，存入字典D中；
> 2. 对每种关系r，解码ST-to-OT得到token对存入集合E中，解码SH-to-OH得到token对并在D中关联其token idx的实体value；
> 3. 对上一步中得到的SH-to-OH token对的所有实体value对，在集合E中依次查询是否其尾token对在E中，进而可以得到三元组信息。
>
> 结合上图的具体case,我们具体描述一下解码过程：
>
> 解码EH-to-ET中得到3个实体：`{New York,New York City,De Blasio}`; 字典D为：`{New:(New York,New York City),De:(De Blasio)}`
>
> 以关系“ **mayor** ”为例,
>
> 1. 解码ST-to-OT得到集合E：`{(City,Blasio)}`;解码SH-to-OH得到`{(New,De)}`，其在字典D中可关联的subject实体集合为`{New York,New York City}`;object集合`{De Blasio}`;
> 2. 遍历上述subject集合和object集合，并在集合E中查询尾token，发现只有一个实体三元组{New York City,mayor,De Blasio}
>
> 以关系“ **born in** ”为例,
>
> 1. 解码 ST-to-OT 得到集合 E：`{(Blasio,York),(Blasio,City)}`; 解码 SH-to-OH 得到 `{(De,New)}`，其在字典D中可关联的subject实体集合为 `{De Blasio}`; object集合为`{New York,New York City}`;
> 2. 遍历上述 subject 集合和 object 集合，并在集合 E 中查询尾 token，可得到 2 个实体三元组：`{De Blasio,born in,New York}` 和 `{De Blasio,born in,New York City}`
>
> 由于关系live in与born in一样，所以我们最终可得到5个三元组：
>
> `(New York City, mayor, De Blasio), (De Blasio, born in, New York), (De Blasio, born in, New York City), (De Blasio, live in, New York), (De Blasio, live in, New York City)`
>
> 其实，只要TPLinker的解码过程，对这篇论文就会有深刻的理解了！大家一定要多看哦～

实验结果：

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210819220014-imagepng)

PS：这个 **HandShaking** 啥意思？？？？

PS2：文中是这么说的：据我们所知，TPLinker是**第一个单阶段联合提取模型**，可以提取各种重叠关系而不受暴露偏差的影响。

### 基于跨度的联合提取模型

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210818150953-imagepng)

论文在此：[1909.07755.pdf (arxiv.org)](https://arxiv.org/pdf/1909.07755.pdf)

话不多说先看摘要：

> 我们介绍了SpERT，一个用于基于跨度的**联合实体和关系提取的注意模型**。我们的主要贡献是对 BERT 嵌入的**轻量级**推理，其特点是实体识别和过滤，以及用一个**本地化的、无标记的上下文表示**进行关系分类。该模型使用强大的**句子内负面样本**进行训练，这些样本在**单一的**BERT通道中被有效地提取。这些方面促进了对句子中**所有跨度的搜索**。在消融研究中，我们证明了预训练、强负向采样和本地化语境的好处。我们的模型在几个数据集上的实体和关系的联合提取方面比以前的工作高出2.6%的F1分数。

那么下面就针对这里说的亮点来记录一下。这里的目的就是解决实体重叠的识别问题、减少运算开销、使用BERT fine-tune。

先看模型图（这个图画的是真的粗糙啊！）

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210818151504-imagepng)

**简述一下训练过程**，首先是使用 BERT 对输入的句子编码，同时还多输出了一个分类标签来表示全局语境（绿色）；然后对于**所有**可能的实体的编码进行池化操作后（红色）拼上位置编码（蓝色，**怎么得到的**）和语境标记（绿色）作为实体分类器的输入（a过程），分类器的输出是实体的类型。

之后对于所有的 span 进行过滤（b过程），比如没有识别出来的、长度超过10的，这样剩下的 span 就会少很多了，得到了实体集合；

之后在这个实体集合里面随机取两个实体，然后呢为了增加上下文信息，把这俩实体之间的句子的编码（黄色）也池化之后跟两个实体拼接一起；这就构成了关系分类器的输入了。关系分类器就根据这些信息来判断关系（c过程）。

这时候**回头看一下摘要**里面的关键词，“联合实体和关系提取的注意模型”达到了；“本地化的、无标记的上下文表示”就是指这个黄色部分的上下文了；“轻量级、单一”呢，指的就是对于每个句子，都只需要通过 BERT 一次（这也算不上优点吧），“所有跨度的搜索”是指在 a 过程中遍历了所有可能的实体并且在 b 过程筛选掉了一些不合适的跨度（span）。

最后就是**句子内负样本**了，暂时没搞明白具体是怎么训练的，强负例就是两个span之间没有任何关系

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210818154454-imagepng)

附上实验结果：

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210818154551-imagepng)

**实验结论**：

1. 通过对比实验发现，全局的上下文信息并没有让关系的分类更加准确，反而让效果变得更差；这表明需要将模型引向所输入句子的相关部分才能取得更好的效果（不至于被干扰）；
2. 负例样本的数量增加到一定程度之后就不会再由效果的提升了，且弱负例样本效果不好；
3. 预训练效果不错；最大池化效果要好过平均池化（为啥要用池化啊）；
4. span 过滤不仅有利于训练和评估速度，而且对SpERT中的准确定位也至关重要（这是通过实验看出来的，原理没说）。

这篇论文最让我惊讶的是还附带了**错误分析**：

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210818161532-imagepng)

图中的示例分别对应了五个不同类型的错误，分别是边界错误、语义错误、无法识别隐式逻辑事实、关系预测错误、数据标注的错误。（PS：个人认为，这应该是所有的实体卦象你提取模型所要面对的问题吧）

**结论**：

> 我们提出了SpERT，一个基于跨度的实体和关系联合提取模型，它依赖于预先训练好的Transformer网络BERT作为其核心。我们的实验表明，通过强负例样本、跨度过滤和本地化的上下文表示，是可以对输入的句子作所有跨度的搜索。我们的结果表明，基于跨度的方法与基于BILOU的模型相比具有竞争力，并且由于其识别重叠实体的能力，可能是未来研究中更有前途的方法。在未来，我们计划研究更详细的关系分类器的上下文形式。目前，我们的模型只是采用了两个实体之间的跨度，这被证明优于完整的上下文。采用额外的 **syntactic features** 或 **learned context**（同时保持高效的穷举搜索）似乎是一个有希望的挑战。

**我的结论**：

文章写的还不错，实验做得很充分，把吹的牛都做出来了，当然还是没有解决相同实体多关系的问题；虽然图画的很丑，但是也太实在了，把错误样本和未来方向全部写出来了，真是个有趣的人！

**更新**：太秀了，我居然还真的发现有人按照作者指出的方向继续做下去了，还超过了原作者的效果并被 COLING 2020 录用了。[Span-based Joint Entity and Relation Extraction with Attention-based Span-specific and Contextual Semantic Representations (aclanthology.org)](https://aclanthology.org/2020.coling-main.8.pdf) 这名字够长的。

那我就继续赶一下，看看这篇“新”论文的亮点在哪里；在此文中，作者明确表明自己是在 SpERT 的基础上继续的，但是在 **span-specific** 和 **contextual semantic representations** 方面有所不同。

> 具体来说，我们的模型通过注意力机制获得这些语义表征。通过计算目标序列语义表征和源序列语义表征之间的匹配程度，注意力机制获得了源序列的注意力分数，这实质上是权重分数。信息越重要，它的权重分数就越高

我前面是不是说 SpERT 的图画的粗糙来着，我后悔了，我收回刚才的话，这个新模型（SPAN）的图更加魔鬼！

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210818210829-imagepng)

跨度分类过滤器分为四个部分（图中红色数字）：

1. 跨度头部和尾部表征的连接（首尾拼接）$\mathcal{H}_{\mathrm{s}}=\left[X_{i} ; X_{i+j}\right]$
2. 跨度特定表征 $\mathcal{F}_{\mathbf{s}}=\sum_{m=i}^{i+j} \text{softmax}(\text{MLP}_{m}(X_{m})) \times X_{\mathbf{m}}$
3. 句子层面的上下文表征 $\mathcal{T}_{\mathbf{s}}=$ Attention $\left(\mathcal{F}_{\mathbf{s}}, \mathcal{B}_{\mathcal{S}}, \mathcal{B}_{\mathcal{S}}\right)$
4. 跨度宽度嵌入（我也不知道这是哪里来的）

关系分类器与过滤器分三个部分（图中蓝色数字）：

1. 关系元组表示的串联，两个实体拼一块儿；$\mathcal{H}_{\mathbf{r}}=\left[\mathbf{F F N N}\left(\mathcal{R}_{s_{1}}\right) ; \mathbf{F} \mathbf{F N N}\left(\mathcal{R}_{s_{2}}\right)\right]$
2. 局部语境表示 $\mathcal{F}_{\mathbf{r}}=$ Attention $\left(\mathcal{H}_{\mathbf{r}}, (X_{m},...,X_{m+n}), (X_{m},...,X_{m+n})\right)$
3. 句子级语境表示 $\mathcal{T}_{\mathbf{r}}=\operatorname{Attention}\left(\mathcal{H}_{\mathbf{r}}, \mathcal{B}_{\mathcal{S}}, \mathcal{B}_{\mathcal{S}}\right)$

这下基本上就已经把这个模型给了解了；整体而言，方法没有变，但是在表征方便还是做了不少创新的，比如把之前的 maxpooling 给换成了注意力模块，maxpooling 让我一脸懵。（就是这个图啊，能不能好好画啊）；

实验结果：

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210818211615-imagepng)

实验结论：

1. 跨域表征（span-specific representation）和句子级的语境表征（sentence-level contextual representation）**效果很好**。
2. 句子的语义表征对关系分类效果明显，但是对实体识别没有帮助甚至会影响效果（似乎很多论文都论证了这个观点），目前可以的解释是这些表示是简介通过反向传播影响实体的识别效果的。

作者结论：

> 我们在基于跨度的实体和关系联合提取方法中引入了基于注意力的语义表示生成方法。我们应用MLP注意力来捕捉跨度特定的特征，旨在获得语义丰富的跨度表征，并通过注意力架构计算特定任务的上下文表征，进一步加强跨度和关系表征。我们的方法在三个基准数据集上牢牢超过了**基于序列标记**和**基于跨度**的SOTA方法，创造了新的最先进的结果。作为未来的工作，我们希望考虑通过减少跨度分类错误来进一步提高关系分类性能。我们还计划探索**更先进的方法来编码有效的跨度和关系表示**。（考虑一下TPLinker）

我的结论：

效果不错，但感觉这个作者是个新手啊，实验不够充分（hhhh），不过也算是继承了一些优点，提供了新的改进方向。

### 级联二进制标签框架

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210819201332-imagepng)

PDF：[A Novel Cascade Binary Tagging Framework for Relational Triple Extraction (aclanthology.org)](https://aclanthology.org/2020.acl-main.136.pdf)

感觉：本文的作者是想要单纯的解决三元组重叠的问题，然后就使用了一个全新的角度来思考关系提取的任务。然后就一个不小心提升了30+%的表现。这篇论文的思路来源应该是苏剑林（本文的二作）这篇文章：[基于DGCNN和概率图的轻量级信息抽取模型 - 科学空间|Scientific Spaces (kexue.fm)](https://kexue.fm/archives/6671)；

> 我们的新框架不是像以前的工作那样把关系当作离散的标签，而是把关系当作函数，把句子中的主体映射到对象，这就自然地处理了重叠的问题。

这里其实就是利用概率的方法来计算：

$$
\begin{aligned}
& \prod_{j=1}^{|D|}\left[\prod_{(s, r, o) \in T_{j}} p\left((s, r, o) \mid x_{j}\right)\right] \\
=& \prod_{j=1}^{|D|}\left[\prod_{s \in T_{j}} p\left(s \mid x_{j}\right) \prod_{(r, o) \in T_{j} \mid s} p\left((r, o) \mid s, x_{j}\right)\right] \\
=& \prod_{j=1}^{|D|}\left[\prod_{s \in T_{j}} p\left(s \mid x_{j}\right) \prod_{r \in T_{j} \mid s} p_{r}\left(o \mid s, x_{j}\right) \prod_{r \in R \backslash T_{j} \mid s} p_{r}\left(o_{\varnothing} \mid s, x_{j}\right)\right]
\end{aligned}

$$

其实简化一下就是：$P(s, p, o)=P(s) P(o \mid s) P(p \mid s, o)$也就是说，我们可以先预测s，然后传入s来预测该s对应的o，然后传入s、o来预测所传入的s、o的关系p，实际应用中，我们还可以把o、p的预测合并为一步，所以总的步骤只需要两步： **先预测s，然后传入s来预测该s所对应的o及p** 。这里使用 sigmoid 激活函数来作为分类；

上面就是思想部分，下面是模型部分：

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210819213014-imagepng)

先预测 s 的起始坐标（PS：这个是不是没法解决实体重叠的问题啊，比如北京大学和北京）：

$$
\begin{array}{r}
p_{i}^{\text {start(s)}}=\sigma\left(\mathbf{W}_{\text {start }} \mathbf{x}_{i}+\mathbf{b}_{\text {start }}\right) \\
p_{i}^{\text {end(s)}}=\sigma\left(\mathbf{W}_{\text {end }} \mathbf{x}_{i}+\mathbf{b}_{\text {end }}\right)
\end{array}

$$

所以 Subject Tagger 的输出是若干个可能的 subject 的坐标；之后根据传入的 s 来预测 s 相对应的 o 和 p：

$$
\begin{array}{r}
p_{i}^{\text {start(o) }}=\sigma\left(\mathbf{W}_{\text {start }}^{r}\left(\mathbf{x}_{i}+\mathbf{v}_{\text {sub }}^{k}\right)+\mathbf{b}_{\text {start }}^{r}\right) \\
p_{i}^{\text {end(o) }}=\sigma\left(\mathbf{W}_{\text {end }}^{r}\left(\mathbf{x}_{i}+\mathbf{v}_{s u b}^{k}\right)+\mathbf{b}_{e n d}^{r}\right)
\end{array}

$$

这里的 $\mathbf{v}_{\text {sub }}^{k}$ 是 subject 的特征，文中的做法是取实体头尾 $\mathbf{x}_{start}$ 与 $\mathbf{x}_{end}$ 的平均（我觉得可以用 Transformer 吧）类似于这篇文章[Span-based Joint Entity and Relation Extraction with Attention-based Span-specific and Contextual Semantic Representations (aclanthology.org)](https://aclanthology.org/2020.coling-main.8.pdf)所采用的方法。

大致思路就是上面的样子了，可以明显看到在实体重叠的部分效果非常好！

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210819215219-imagepng)

实验结论：

> 在本文中，我们介绍了一个新颖的级联二元标签框架（CASREL），它来自于关系三元组提取的原则性问题表述。我们没有将关系建模为实体对的离散标签，而是将关系建模为将 subject 映射到 object 的函数，这为重新审视关系三元组的提取任务提供了一个新视角。因此，我们的模型可以同时从句子中提取多个关系三元组，而不会出现重叠的问题。我们在两个广泛使用的数据集上进行了广泛的实验，以验证拟议的CASREL框架的有效性。实验结果表明，我们的模型在不同的场景下都**有压倒性的优势**，特别是在提取重叠的关系三元组方面。

### 实体与关系分别编码的 Table-Sequence 编码器

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210820205910-imagepng)

论文：[Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders - ACL Anthology](https://aclanthology.org/2020.emnlp-main.133/)

**动机**：作者认为目前很多联合提取模型都是一个填表的问题（我发现这样做的人不多啊），使用一个单一的编码器来完成实体提取和关系提取的任务。所以我要**改变**！

所以作者就把原本的两个任务拆成了两个任务，把一个编码器换成了两个编码器，后面的一些列操作基本都是为了这个想法服务的；

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210820210525-imagepng)

一个是序列标注的任务，使用了序列编码器；一个是关系填表任务，使用关系编码器；但是作者也清楚两个任务之间的部分特征共享又是有利于两个任务的，所以就藕断丝连，在他们之间又加了交互的模块；后来，作者发现在编码的时候插入预训练模型的权重也能提升效果，就加上了预训练权重模块；综上所述就有了此篇论文的几个贡献点：

1. 学习两个编码器，独立工作但是相互交互；
2. 使用多维递归神经网络（MD-RNN）来提取关系表的结构信息；
3. 有效的利用的 BERT 的注意力权重中附带的词汇交互信息；

当然从上面的任务中也能看出来这个模型依旧是存在实体关系重叠的问题，所以并没有使用 NYT 和 WebNLG 数据集；

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210820211942-imagepng)

整个模型可以分为四个部分，对输入的处理（蓝色部分）、表编码器（红色部分）、序列编码器（黄色部分）、预训练注意力权重（虚线部分）；

首先对于**第一部分**输入的处理，每个词的编码包括了 WordEmbeddeding（来自LSTM）、CharEmbedding（来自LSTM）、词的语境编码（来自BERT）；把他们按照特征的维度拼在一起之后经过一层线性映射就得到了初始输入；

$$
\boldsymbol{S}_{0} = \operatorname{Linear}\left( \left[ \boldsymbol{x}^{c}, \boldsymbol{x}^{w}, \boldsymbol{x}^{\ell} \right] \right)

$$

接下来就是**第二部分**表编码器，表编码器和序列编码器都是多层的，每一层的输入都有三个：上一层的输入 $T_{l-1}$、上一层序列编码器的输入 $S_{l-1}$ 以及预训练模型的注意力权重 $T^{\ell} \in \R^{N\times N \times (L^{\ell} \times A^{\ell})} $（后面介绍）；但是这里面的处理非常的复杂，真不知道作者是怎么想到的！如右图所示，表编码器的第一步是把输入拼成一个表，$\boldsymbol{X}$ 表示模型中的表，计算方法是：

$$
X_{l,i,j} = \operatorname{ReLU} \left( \operatorname{Linear} \left( [S_{l-1, i}; S_{l-1, j}; T^{\ell}_{i, j}] \right) \right)

$$

得到这个表之后就可以把这个表和上一层的输入同时放到 MD-RNN 中，这里的多维循环神经网络会同时考虑 $i$ 、$j$ 以及上一层的输出这三个维度的信息；同时为了获取表的双向信息，需要同时考虑多个情况，如下图所示，最终基于计算和实验效果的考虑选择结合（a）和（c）两种方式的信息作为上下文信息。

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210820214905-imagepng)

计算公式如下：

$$
\begin{aligned}
&T_{l, i, j}^{(a)}=\operatorname{GRU}^{(a)}\left(X_{l, i, j}, T_{l-1, i, j}^{(a)}, T_{l, i-1, j}^{(a)}, T_{l, i, j-1}^{(a)}\right) \\
&T_{l, i, j}^{(c)}=\operatorname{GRU}^{(c)}\left(X_{l, i, j}, T_{l-1, i, j}^{(c)}, T_{l, i+1, j}^{(c)}, T_{l, i, j+1}^{(c)}\right) \\
&T_{l, i, j}=\left[T_{l, i, j}^{(a)} ; T_{l, i, j}^{(c)}\right]
\end{aligned}

$$

看完了复杂的表编码器，就可以看看**第三部分**序列编码器了，序列编码器是想要使用类似于 self-attention 模块的，但作者一系列脑洞把我搞晕了，按照正常的逻辑：$T_{l}$ 应该是作为 Q 传入注意力模块，K 和 V 是 $S_{l-1}$；输出是加权后的 $S_{l-1}$；

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210820220130-imagepng)

然后作者就觉得原本的 scaled dot-product attention（$Q_{i} \times K_{j}$）不够好，就把点积操作替换成了这个评分函数 $f\left(Q_{i}, K_{j}\right)=U \cdot g\left(Q_{i}, K_{j}\right)$，其中 $g\left(Q_{i}, K_{j}\right)=\tanh \left(Q_{i} W_{0}+K_{j} W_{1}\right)$ ，$U$ 是一个可学习的向量；**但是**后来作者一想，Q 和 K 本来就是一样的，都是 $S_{l-1}$ ，那还见什么外啊，这个 $g$ 的本质不就是对 $S_{l-1}$ 做非线性映射吗！那这巧了不是，$T_{l, i, j}$ 这不就是现成的吗？所以最终的注意力模块就变成了这样：

$$
\operatorname{SelfAttn}(\boldsymbol{S}_{l-1}) = \operatorname{softmax}(U \cdot T_{l} ) \times \boldsymbol{S}_{l-1}

$$

那么整个序列编码器的输出就变成了：

$$
\begin{aligned}
\tilde{\boldsymbol{S}}_{l} &=\operatorname{LayerNorm}\left(\boldsymbol{S}_{l-1}+\operatorname{SelfAttn}\left(\boldsymbol{S}_{l-1}\right)\right) \\
\boldsymbol{S}_{l} &=\operatorname{LayerNorm}\left(\tilde{\boldsymbol{S}}_{l}+\mathrm{FFNN}\left(\tilde{\boldsymbol{S}}_{l}\right)\right)
\end{aligned}

$$

**第四部分**就是预训练模型中的注意力权重了，文中只是简单的这么介绍的：

> 我们将所有 head 和各层的注意力权重叠加在一起形成 $T^{\ell} \in \R^{N\times N \times (L^{\ell} \times A^{\ell})} $ ，其中 $L^{\ell}$ 表示 Transformer 堆叠的层数，$A^{\ell}$ 表示每一层的 head 的数量。

呼~终于把这四个部分介绍完了！

**实验细节**

论文中作者说是利用两个编码器的输出去推断实体和关系的概率分布，具体计算公式就是下面这坨：

$$
\begin{aligned}
P_{\theta}\left(\boldsymbol{Y}^{\mathrm{NER}}\right) &=\operatorname{softmax}\left(\operatorname{Linear}\left(\boldsymbol{S}_{L}\right)\right) \\
P_{\theta}\left(\boldsymbol{Y}^{\mathrm{RE}}\right) &=\operatorname{softmax}\left(\operatorname{Linear}\left(\boldsymbol{T}_{L}\right)\right)
\end{aligned}

$$

其中 $\boldsymbol{Y}$ 表示预测标签的随机变量（我不理解，是指表吗？那外表面的 P 又是什么鬼？），具体的损失函数就是下面这坨：

$$
\begin{aligned}
\mathcal{L}^{\mathrm{NER}} &=\sum_{i \in[1, N]}-\log P_{\theta}\left(Y_{i}^{\mathrm{NER}}=y_{i}^{\mathrm{NER}}\right) \\
\mathcal{L}^{\mathrm{RE}} &=\sum_{i, j \in[1, N] ; i \neq j}-\log P_{\theta}\left(Y_{i, j}^{\mathrm{RE}}=y_{i, j}^{\mathrm{RE}}\right)
\end{aligned}

$$

我不理解！按照作者意思就是，整个模型的输出就是两个概率分布函数，可是你这个函数的输入是什么啊？怎么计算啊？按理说你得到的应该是个表和一个序列啊，所以这里的 $\boldsymbol{Y}$ 应该是个表才对啊，就类似于文章的一开头的那个表啊！

我不管了，模型大致就这个意思，毕竟咱们看看实验就知道了！

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210820224117-imagepng)

从实验结果可以看到，其实模型的提升都不是很明显，很多数据集上仅仅只是 **1%** 左右的提升，对于提升最为明显的 ACE05 来说，在没有使用预训练注意力权重的时候达到了 **64.8%** 了，也仅仅只是提升了 **1.4%**；**67.6%**的数据表现是从**预训练注意力权重**中得到的！虽然后续的消融实验中可以看到使用双编码器是对最终的效果有帮助的，但是从提升的效果上面来看，模型中复杂的模型编码方式并不是个很好的选择；

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210820224921-imagepng)

作者做了很多的实验，但是都没有太多的说服力；只是能从某方面证明自己的模型的可行性，并没什么特点，比如这里对于模型层数的实验结果，可以说是一个有趣的发现，但是作者并没有做出更深入的详细分析，也就变得没什么价值了，

**作者结论**：

> 在本文中，我们介绍了用于联合提取实体及其关系的新型 table-sequence 编码器架构。它学习两个独立的编码器，而不是“一个”编码器，这两个编码器之间存在明确的相互作用。我们还介绍了一种新的方法，以有效地利用预先训练好的语言模型所捕获的有用信息来完成这种涉及到表格表示的联合学习任务。我们在四个标准数据集的NER和RE任务中都取得了最先进的F1分数，这证实了我们方法的有效性。在未来，我们希望研究如何将表格表示应用于其他任务。另一个方向是将表和序列的交互方式推广到其他类型的表示中。

我的结论：

作者虽然实验数据表现并没有很大的提升，而且也并没有解决实体重叠的问题；但是作者通过实验论证了两个编码器的效果可能会比一个编码器要好；使用预训练模型中的注意力权重作为模型的嵌入可能会有更好的效果。

## 参考资料

太多了！
