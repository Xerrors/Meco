---
title: 实现对服务器的Markdown文章的搜索与解析
date: 2021-08-10 19:21:01
permalink: /search-and-parse-md-file-from-server/
cover:
tags:
- 算法
- 教程
- 开发
categories: 算法
---
效果如下

![图片](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20210824184237-imagepng)

## 后端算法

简单描述一下这个需求，首先我的所有文章都是以 markdown 的形式保存在我的服务器上面，通过 vuepress 渲染之后部署到博客网站的。同时这些文章的基本信息都是保存在数据库里面的。

所以一个简单的思路就是当服务器接受到要查询的字符串 `QueryString` 的时候，遍历所有文章，如果改文章中出现了这个字符串，就把该 `QueryString` 的前后上下文文本发回浏览器，浏览器渲染一下就行。

但是所存在的问题还是很多的：

1. 在查询的时候可能需要多关键词查询，比如使用空格隔开，查询“测试 开发”；
2. 一篇文章中可能会存在多个关键词，此时需要将这些关键词的上下文全部找到；
3. 因为文章是使用 markdown 写的，所以上下文中可能会存在大量的符号信息，比如图片链接 `![txt](url)`、超链接 `[txt](url)`、加粗 `**`，标题的 `#` 等；

问题是有的，办法也是有的。

首先针对于问题1和2，很好办，我们就检查每一个关键词呗，只有当一篇文章同时出现所有关键词的时候才认为是结果；之后针对这篇文章，把每个关键词的所有出现的地方的上下文全部找出来，上下文的选取规则是该词的前15个字符和后100个字符，当然这里要留意越界问题。

得到这些片段之后，将相互有重叠的片段拼接在一起，作为一个上下文片段；那么我们最终应该给前端的数据格式应该是这样的：

```json
[
  {
    "path": "aaa",
    "title": "bbb",
    "tags": ["a", "b"],
    "res": ["片段1", "片段2"]
  },
  {
    "path": "ddd",
    "title": "ccc",
    "tags": ["v", "a"],
    "res": ["片段1", "片段2"]
  }
]
```

那么对于问题 3，就需要熟练地使用正则表达式来完成对文本数据的处理啦！

### 详细步骤

提示：这里的代码只是为了辅助理解，一些不必要的代码并没有贴出来，如果想要看完整的代码，可以直接拉到最后面。

步骤一：遍历数据库中的所有文章，并对文章进行清洗；

上面是对文本进行清洗，下面的 md 是使用 frontmatter 解析之后得到的对象；

```python
for page in pages:
    with open(page.local_path) as f:
        t = f.read()
        replaced_str = re.sub(r"---[\s\S]*?---", " ", t) # 去除 frontmatter
        replaced_str = re.sub(r"```([\s\S]*?)```", " ", replaced_str) # 代码块
        replaced_str = re.sub(r"!\[([^\]]*?)\]\(([\s\S]*?)\)", " ", replaced_str) # 图片
        replaced_str = re.sub(r"\[([^\]]*?)\]\(([\s\S]*?)\)", "\g<1>", replaced_str) # 链接
        replaced_str = re.sub(r"\(?(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]\)?", " ", replaced_str) # 地址
        replaced_str = re.sub(r"`{1,2}([^`](.*?))`{1,2}", "\g<1>", replaced_str) # 行内代码
        replaced_str = re.sub(r"\$\$[\s\S]*?\$\$", " ", replaced_str) # 公式
        replaced_str = re.sub(r"\$([\s\S]*?)\$", "\g<1>", replaced_str) # 行内公式
        replaced_str = re.sub(r"""<(?:[^"'>]|(["'])[^"']*\1)*>""", " ", replaced_str) # HTML 代码
        replaced_str = re.sub(r"(\*\*|__)(.*?)(\*\*|__)", "\g<2>", replaced_str) # 加粗
        replaced_str = re.sub(r"\~\~(.*?)\~\~", "\g<1>", replaced_str) # 删除
        replaced_str = re.sub(r"(#+) (.*)", " \g<2> ", replaced_str) # 标题
        replaced_str = re.sub(r"(>+) (.*)", " \g<2> ", replaced_str) # 引用
        replaced_str = re.sub(r" *\n+[-\*\+]+ (.*)", " \g<1> ", replaced_str) # 无序列表
        replaced_str = re.sub(r" *\n+[0-9]+\. (.*)", " \g<1> ", replaced_str) # 有序列表
        replaced_str = re.sub(r"[\n\r\s]+", " ", replaced_str) # 空字符

        md = parse_markdown(t, True)

        ……略
```

之后就是对于每一篇文章，先检测一下这篇文章有没有所有的关键词，这里的 fromtmatter_to_text 是一个把 title、tags、categories 拼成一个字符串的函数，就不贴出来了。

```python
for page in pages:
    with open(page.local_path) as f:

        ……上面已经出现略

        # 对于使用空格隔开的关键词，策略是当整个文章中同时出现所有的关键词的时候才做匹配
        # 关键词多个关键词必须
        full_text = fromtmatter_to_text(md) + replaced_str

        if not case_sensitive:
            full_text = full_text.lower()

        flag = True
        for key in keywords:
            if key not in full_text:
                flag = False
                continue

        if not flag:
            continue
```

如果文章中存在所有的关键词的话，就开始下一步切割了，这里使用正则的 `finditer` 来查找所有的关键词的坐标的起始位置，之后我们将这些关键词的前后加上起始标志和结束标志 `$START$`、`$END$`，这是为了后面前端做高亮使用的。再然后我们将之前的关键词坐标转换成片段的坐标，对于有重叠的片段要合并；整个过程的坐标变换有点复杂，看着也比较乱。

1. 找到关键词的位置；
2. 嵌入标志符（注意，嵌入标志符之后，原本找到的坐标位置就发生了改变，这里需要进行一次转换）；
3. 把位置坐标转换为片段坐标；
4. 把有重叠的片段坐标合并；

不管理不理解，实现方法就是下面这样：

```python
def get_slices_by_multi_keywords(keywords, text, pl=15, el=100):
    import re

    pt, et = '$START$', '$END$'
    l_pt = len(pt)
    l_et = len(et)

    spans = [i.span() for i in re.finditer(r"|".join(keywords), text.lower())]

    spans_new = []
    for i in range(len(spans)):
        pre_add = i * (l_pt + l_et)
        t_span = (spans[i][0]+pre_add, spans[i][1]+pre_add)
        spans_new.append([t_span[0]+l_pt, t_span[1]+l_pt])
        text = text[:t_span[0]] + pt + text[t_span[0]:t_span[1]] + et + text[t_span[1]:]

    strs = [(max(0, span[0]-pl-l_pt), min(len(text), span[1]+el+l_et)) for span in spans_new]

    if len(strs) == 1:
        return [text[strs[0][0]:strs[0][1]]]

    # 这一段我觉得写的很烂，但是挺好理解的，暂时先这么做看看
    new_s = []
    tmp = None
    for i in range(len(strs)):
        if not tmp:
            tmp = list(strs[i])

        if i != len(strs) -1 and tmp[1] > strs[i+1][0]:
            tmp[1] = strs[i+1][1]
        else:
            new_s.append(text[tmp[0]:tmp[1]])
            tmp = None
  
    return new_s
```

### 后端代码

相较于上面而言，这里又加入了大小写敏感的检测。

```python
def parse_markdown(markdown_data, isStr=False):
    if isStr:
        md = frontmatter.loads(markdown_data)
    else:
        with open(markdown_data, encoding='UTF-8') as f:
            md = frontmatter.load(f)
    return md

def fromtmatter_to_text(md):
    """把文章的标题、标签、分类信息连接成一个字符串"""
    if type(md['categories']) == list:
        categories = " ".join(md['categories'])
    elif type(md['categories']) == str:
        categories = md['categories']
    else:
        categories = ""

    return " ".join([md['title'], " ".join(md['tags']), categories])


def get_slices_by_multi_keywords(keywords, text, pl=15, el=100, case_sensitive=False):
    import re

    pt, et = '$START$', '$END$'
    l_pt = len(pt)
    l_et = len(et)

    if case_sensitive:
        spans = [i.span() for i in re.finditer(r"|".join(keywords), text)]
    else:
        spans = [i.span() for i in re.finditer(r"|".join(keywords), text.lower())]

    spans_new = []
    for i in range(len(spans)):
        pre_add = i * (l_pt + l_et)
        t_span = (spans[i][0]+pre_add, spans[i][1]+pre_add)
        spans_new.append([t_span[0]+l_pt, t_span[1]+l_pt])
        text = text[:t_span[0]] + pt + text[t_span[0]:t_span[1]] + et + text[t_span[1]:]

    strs = [(max(0, span[0]-pl-l_pt), min(len(text), span[1]+el+l_et)) for span in spans_new]

    if len(strs) == 1:
        return [text[strs[0][0]:strs[0][1]]]

    # 这一段我觉得写的很烂，但是挺好理解的，暂时先这么做看看
    new_s = []
    tmp = None
    for i in range(len(strs)):
        if not tmp:
            tmp = list(strs[i])

        if i != len(strs) -1 and tmp[1] > strs[i+1][0]:
            tmp[1] = strs[i+1][1]
        else:
            new_s.append(text[tmp[0]:tmp[1]])
            tmp = None
  
    return new_s


def search_alg(text, case_sensitive=False):
    import re
    from app.tables import LocalArticlesTable
    pages = LocalArticlesTable.query.all()
    result = []

    if case_sensitive:
        text = text.strip()
    else:
        text = text.lower().strip()

    keywords = text.split()

    for page in pages:
        with open(page.local_path) as f:
            t = f.read()
            replaced_str = re.sub(r"---[\s\S]*?---", " ", t) # 去除 frontmatter
            replaced_str = re.sub(r"```([\s\S]*?)```", " ", replaced_str) # 代码块
            replaced_str = re.sub(r"!\[([^\]]*?)\]\(([\s\S]*?)\)", " ", replaced_str) # 图片
            replaced_str = re.sub(r"\[([^\]]*?)\]\(([\s\S]*?)\)", "\g<1>", replaced_str) # 链接
            replaced_str = re.sub(r"\(?(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]\)?", " ", replaced_str) # 地址
            replaced_str = re.sub(r"`{1,2}([^`](.*?))`{1,2}", "\g<1>", replaced_str) # 行内代码
            replaced_str = re.sub(r"\$\$[\s\S]*?\$\$", " ", replaced_str) # 公式
            replaced_str = re.sub(r"\$([\s\S]*?)\$", "\g<1>", replaced_str) # 行内公式
            replaced_str = re.sub(r"""<(?:[^"'>]|(["'])[^"']*\1)*>""", " ", replaced_str) # HTML 代码
            replaced_str = re.sub(r"(\*\*|__)(.*?)(\*\*|__)", "\g<2>", replaced_str) # 加粗
            replaced_str = re.sub(r"\~\~(.*?)\~\~", "\g<1>", replaced_str) # 删除
            replaced_str = re.sub(r"(#+) (.*)", " \g<2> ", replaced_str) # 标题
            replaced_str = re.sub(r"(>+) (.*)", " \g<2> ", replaced_str) # 引用
            replaced_str = re.sub(r" *\n+[-\*\+]+ (.*)", " \g<1> ", replaced_str) # 无序列表
            replaced_str = re.sub(r" *\n+[0-9]+\. (.*)", " \g<1> ", replaced_str) # 有序列表
            replaced_str = re.sub(r"[\n\r\s]+", " ", replaced_str) # 空字符

            md = parse_markdown(t, True)


            # 对于使用空格隔开的关键词，策略是当整个文章中同时出现所有的关键词的时候才做匹配
            # 关键词多个关键词必须
            full_text = fromtmatter_to_text(md) + replaced_str

            if not case_sensitive:
                full_text = full_text.lower()

            flag = True
            for key in keywords:
                if key not in full_text:
                    flag = False
                    continue

            if not flag:
                continue

            res_dict = {
                "title": md["title"],
                "path": md["permalink"],
                "tags": md["tags"],
                "res": []
            }

            res_dict['res'] = get_slices_by_multi_keywords(keywords, replaced_str, case_sensitive=case_sensitive)
            result.append(res_dict)
  
    return result
```

## 关于前端

前端部分似乎并没有什么可说的，无非就是将搜索框中的数据发给服务器，然后接受服务器的数据，把这些数据渲染成卡片就行了。

唯一要说的应该就是高亮了；也很简单，

我们回顾一下前面的数据格式：

```json
[
  {
    "path": "aaa",
    "title": "bbb",
    "tags": ["a", "b"],
    "res": ["片段1", "片段2"]
  },
  {
    "path": "ddd",
    "title": "ccc",
    "tags": ["v", "a"],
    "res": ["片段1", "片段2"]
  }
]
```

我们解析一下每个结果里面的 res，把里面的标志符换成 HTML 标签，之后让 vue 渲染 v-html 就行。

```javascript
function parseHTML (result:string[]) {
  return result.map((item:string) => {
    return item.replaceAll("$START$", '<span style="background: #c2e5f8; padding: 0 2px; border-radius: 4px;">')
               .replaceAll("$END$", '</span>') + "……";
  })
}

/*
在 vue 中使用就可以了

<p v-for="(t, ind) in res.res" :key="ind" v-html="t"></p>

/*
```
